{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location": "/archive/", "text": "news archive2020 sep 11 2020 google summer of code 2020 may 14 2020 replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 feb 12 2020 google summer of code julia summer of code 2019 dec 14 2019 turing s blog", "title": "Articles"},{"location": "/feed.xml", "text": "turing jl turing a robust efficient and modular library for general purpose probabilistic programming v0 14 2 mon 14 sep 2020 00 59 00 0000 mon 14 sep 2020 00 59 00 0000 jekyll v4 1 1 google summer of code 2020 lt p gt as the 2020 lt a href quot https summerofcode withgoogle com quot gt google summer of code lt a gt comes to a close the turing team thought it would be a good opportunity to reflect on the work that was done by our superb students this summer lt p gt lt p gt lt a href quot https github com saranjeetkaur quot gt saranjeet kaur lt a gt s lt a href quot https summerofcode withgoogle com projects 6567464390885376 quot gt project lt a gt focused primarily on expanding lt a href quot https github com turinglang nestedsamplers jl quot gt nestedsamplers jl lt a gt nestedsamplers jl now supports lt a href quot https arxiv org abs 1506 00171 quot gt polychord style lt a gt nested sampling natively which is an absolute delight saranjeet wrote about this lt a href quot https nextjournal com saranjeet kaur extending nestedsamplersjl quot gt here lt a gt she also provided a good tutorial on how to use nestedsamplers jl lt a href quot https nextjournal com saranjeet kaur illustrations of use of nestedsamplersjl quot gt here lt a gt the nestedsamplers jl integration with turing is still on going integrating new samplers with turing is one of the more difficult tasks if you are interested to see the progress on this check out the relevant lt a href quot https github com turinglang turing jl pull 1333 quot gt pull request lt a gt lt p gt lt p gt lt a href quot https github com luiarthur quot gt arthur lui lt a gt s lt a href quot https summerofcode withgoogle com projects 5861616765108224 quot gt project lt a gt was to provide a much needed set of benchmarks of bayesian nonparametric models between turing and other ppls arthur s work spawned a lt a href quot https github com luiarthur turingbnpbenchmarks quot gt github repository lt a gt with good practices for benchmarking as well as three blog posts with some very cool statistics on turing s performance lt p gt lt ol gt lt li gt lt a href quot https luiarthur github io turingbnpbenchmarks dpsbgmm quot gt dirichlet process gaussian mixture model via the stick breaking construction in various ppls lt a gt lt li gt lt li gt lt a href quot https luiarthur github io turingbnpbenchmarks gp quot gt gaussian process regression model in various ppls lt a gt lt li gt lt li gt lt a href quot https luiarthur github io turingbnpbenchmarks gpclassify quot gt gaussian process classification model in various ppls lt a gt lt li gt lt ol gt lt p gt finally lt a href quot https github com sharanry quot gt sharan yalburgi lt a gt a returning gsoc student completed an epic amount of work turing s growing suite of lt a href quot https summerofcode withgoogle com projects 5565948129443840 quot gt gaussian process tools lt a gt in particular the github organization lt a href quot https github com juliagaussianprocesses quot gt juliagaussianprocesses lt a gt was founded and serves as an effort to build a robust gaussian process framework for the julia ecosystem the framework consists of multiple gp related julia packages lt p gt lt ul gt lt li gt lt a href quot https github com juliagaussianprocesses kernelfunctions jl quot gt kernelfunctions jl lt a gt provides kernel functions for gps as well as efficient ad for these kernels kernelfunctions jl also supports multi output gps by providing necessary data abstractions and multi output kernels lt li gt lt li gt lt a href quot https github com juliagaussianprocesses abstractgps jl quot gt abstractgps jl lt a gt defines gp abstractions and provides exact posteriors it provides support for induced points based gp posteriors and for efficient sequential online sparse gp updates lt li gt lt li gt lt a href quot https github com juliagaussianprocesses gplikelihoods jl quot gt gplikelihoods jl lt a gt defines alternate likelihoods for non gaussian gps lt li gt lt li gt lt a href quot https github com juliagaussianprocesses gpmlj jl quot gt gpmlj jl lt a gt provides a julia interface for lt a href quot https github com gpflow gpflow quot gt gpflow lt a gt a gp library written in python using tensorflow lt li gt lt ul gt lt p gt special thanks to our three gsoc students for this summer who all did excellent work additional thanks to google for supporting open source software development and the julia language lt p gt fri 11 sep 2020 00 00 00 0000 v0 14 2 posts 2020 09 11 gsoc v0 14 2 posts 2020 09 11 gsoc replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 lt p gt the turing jl team is currently exploring possibilities in an attempt to help with the ongoing sars cov 2 crisis as preparation for this and to get our feet wet we decided to perform a replication study of the lt a href quot https www imperial ac uk mrc global infectious disease analysis covid 19 report 13 europe npi impact quot gt imperial report 13 lt a gt which attempts to estimate the real number of infections and impact of non pharmaceutical interventions on covid 19 in the report the inference was performed using the probabilistic programming language ppl stan we have explicated their model and inference in turing jl a julia based ppl we believe the results and analysis of our study are relevant for the public and for other researchers who are actively working on epidemiological models to that end our implementation and results are available lt a href quot https github com cambridge mlg covid19 quot gt here lt a gt lt p gt lt p gt in summary we replicated the imperial covid 19 model using turing jl subsequently we compared the inference results between turing and stan and our comparison indicates that results are reproducible with two different implementations in particular we performed 4 sets of simulations using the imperial covid 19 model the resulting estimates of the expected real number of cases in contrast to the lt em gt recorded lt em gt number of cases the reproduction number r t and the expected number of deaths as a function of time and non pharmaceutical interventions npis for each simulation are shown below lt p gt lt div id quot simulation 1 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full prior json quot function err fig plotly plot quot simulation 1 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation a lt strong gt hypothetical simulation from the model without data prior predictive or non pharmaceutical interventions under the prior assumptions of the imperial covid 19 model there is a very wide range of epidemic progressions with expected cases from almost 0 to 100 of the population over time the black bar corresponds to the date of the last observation note that r t has a different time range than the other plots following the original report this shows the 100 days following the country specific lt code class quot language plaintext highlighter rouge quot gt epidemic start lt code gt which is defined to be 31 days prior to the first date of 10 cumulative deaths while the other plots show the last 60 days lt p gt lt div id quot simulation 2 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full posterior json quot function err fig plotly plot quot simulation 2 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation b lt strong gt future simulation with non pharmaceutical interventions kept in place posterior predictive after incorporating the observed infection data we can see a substantially more refined range of epidemic progression the reproduction rate estimate lies in the range of 3 5 5 6 before any intervention is introduced the dotted lines correspond to observations and the black bar corresponds to the date of the last observation lt p gt lt div id quot simulation 3 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full counterfactual json quot function err fig plotly plot quot simulation 3 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation c lt strong gt future simulation with non pharmaceutical interventions removed now we see the hypothetical scenarios after incorporating infection data but with non pharmaceutical interventions removed this plot looks similar to simulation a but with a more rapid progression of the pandemic since the estimated reproduction rate is bigger than the prior assumptions the dotted lines correspond to observations and the black bar corresponds to the date of the last observation lt p gt lt div id quot simulation 4 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full counterfactual2 json quot function err fig plotly plot quot simulation 4 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation d lt strong gt future simulation with when lt code class quot language plaintext highlighter rouge quot gt lockdown lt code gt is lifted two weeks before the last observation predictive posterior as a result there is a clear rapid rebound of the reproduction rate comparing with simulation b we do not observe an lt em gt immediate lt em gt increase in the number of expected cases and deaths upon lifting lockdown but there is a significant difference in the number of cases and deaths in the last few days in the plot simulation d results in both greater number of cases and deaths as expected this demonstrates how the effects of lifting an intervention might not become apparent in the measurable variables e g deaths until several weeks later the dotted lines correspond to observations the black bar corresponds to the date of the last observation and the red bar indicates when lt code class quot language plaintext highlighter rouge quot gt lockdown lt code gt was lifted lt p gt lt p gt overall simulation a shows the prior modelling assumptions and how these prior assumptions determine the predicted number of cases etc before seeing any data simulation b predicts the trend of the number of cases etc using estimated parameters and by keeping all the non pharmaceutical interventions in place simulation c shows the estimate in the case where none of the intervention measures are ever put in place simulation d shows the estimates in the case when the lockdown was lifted two weeks prior to the last observation while keeping all the other non pharmaceutical interventions in place lt p gt lt p gt we want to emphasise that we do not provide additional analysis of the imperial model yet nor are we aiming to make any claims about the validity or the implications of the model instead we refer to imperial report 13 for more details and analysis the purpose of this post is solely to add validation to the lt em gt inference lt em gt performed in the paper by obtaining the same results using a different probabilistic programming language ppl and by exploring whether or not turing jl can be useful for researchers working on these problems lt p gt lt p gt for our next steps we re looking at collaboration with other researchers and further developments of this and similar models there are some immediate directions to explore lt p gt lt ol gt lt li gt incoporation of more sources of data e g national mobility seasonal changes and behavior changes in individuals lt li gt lt li gt how the assumptions incorporated into the priors and their parameters change resulting posterior lt li gt lt li gt the current model does not directly include recovery as a possibility and assumes that if a person has been infected once then he she will be infectious until death number of recovered cases suffers from the same issues as the number of cases it cannot be directly observed but we can also deal with it in a similar manner as is done with number of cases and incorporate this into the model for a potential improvement this will result in a plethora of different models from which we can select the most realistic one using different model comparions techniques e g leave one out cross validation loo cv lt li gt lt ol gt lt p gt such model refinement can be potentially valuable given the high impact of this pandemic and the uncertainty and debates in the potential outcomes lt p gt lt p gt lt strong gt acknowledgement lt strong gt lt em gt we would like to thank the julia community for creating such an excellent platform for scientific computing and for the continuous feedback that we have received we also thank researchers from computational and biological laboratory at cambridge university for their feedback on an early version of the post lt em gt lt footnotes gt lt p gt thu 14 may 2020 00 00 00 0000 v0 14 2 posts 2020 05 04 imperial report13 analysis v0 14 2 posts 2020 05 04 imperial report13 analysis google summer of code julia summer of code lt p gt last year turing participated in the google summer of code gsoc through the julia language organization it was a fun time and the project was better for it turing plans to participate in the upcoming gsoc and we wanted to outline some potential projects and expectations we have for applicants lt p gt lt p gt if you are not aware google provides funds to students around the world to develop a project of their choice over the summer students receive funds from google and spend three months on any open source project lt p gt lt p gt the turing development team has prepared a list of possible projects that we have deemed valuable to the project and easy enough that it could feasibly be created in the three month limit this list is not exlusive if you have a good idea you can write it up in your proposal though it is recommend that you reach out to any of the turing team on julia s lt a href quot https julialang slack com quot gt slack lt a gt you can get an invite lt a href quot https slackinvite julialang org quot gt here lt a gt or lt a href quot https discourse julialang org c domain probprog quot gt discourse lt a gt messages on discourse should be posted to the probabilistic programming category we ll find you lt p gt lt p gt possible project ideas lt p gt lt ul gt lt li gt lt strong gt benchmarking lt strong gt turing s performance has been sporadically benchmarked against various other probabilistic programming languages e g turing stan pymc3 tensorflow prob but a systemic approach to studying where turing excels and where it falls short would be useful a gsoc student would implement identical models in many ppls and build tools to benchmark all ppls against one another lt li gt lt li gt lt strong gt nested sampling integration lt strong gt turing focuses on modularity in inference methods and the development team would like to see more inference methods particularly the popular nested sampling method a julia package lt a href quot https github com mileslucas nestedsamplers jl quot gt nestedsamplers jl lt a gt but it is not hooked up to turing and does not currently have a stable api a gsoc student would either integrate that package or construct their own nested sampling method and build it into turing lt li gt lt li gt lt strong gt automated function memoization by model annotation lt strong gt function memoization is a way to reduce costly function evaluation by caching the output when the same inputs are given turing s gibbs sampler often ends up lt a href quot https turing ml dev docs using turing performancetips reuse computations in gibbs sampling quot gt rerunning expensive functions lt a gt multiple times and it would be a significant performance improvement to allow turing s model compiler to automatically memoize functions where appropriate a student working on this project would become intimately familiar with turing s model compiler and build in various automated improvements lt li gt lt li gt lt strong gt making distributions gpu compatible lt strong gt julia s gpu tooling is generally quite good but currently turing is not able to reliably use gpus while sampling because lt a href quot https github com juliastats distributions jl quot gt distributions jl lt a gt is not gpu compatible a student on this project would work with the turing developers and the distributions developers to allow the use of gpu parallelism where possible in turing lt li gt lt li gt lt strong gt static distributions lt strong gt small fixed size vectors and matrices are fairly common in turing models this means that sampling in turing can probably benefit from using statically sized vectors and matrices from lt a href quot https github com juliaarrays staticarrays jl quot gt staticarrays jl lt a gt instead of the dynamic normal julia arrays beside the often superior performance of small static vectors and matrices static arrays are also automatically compatible with the gpu stack in julia currently the main obstacle to using staticarrays jl is that distributions in lt a href quot https github com juliastats distributions jl quot gt distributions jl lt a gt are not compatible with staticarrays a gsoc student would adapt the multivariate and matrix variate distributions as well as the univariate distribution with vector parameters in distributions jl to make a spin off package called staticdistributions jl the student would then benchmark staticdistributions jl against distributions jl and showcase an example of using staticdistributions jl together with lt a href quot https github com juliagpu cuarrays jl quot gt cuarrays jl lt a gt and or lt a href quot https github com juliagpu cudanative jl quot gt cudanative jl lt a gt for gpu acceleration lt li gt lt li gt lt strong gt gpnet extensions lt strong gt one of turing s sattelite packages lt a href quot https github com turinglang gpnet jl quot gt gpnet lt a gt is designed to provide a comprehensive suite of gaussian process tools see lt a href quot https github com turinglang gpnet jl issues 2 quot gt this issue lt a gt for potential tasks there s a lot of interesting stuff going on with gps and this task in particular may have some creative freedom to it lt li gt lt li gt lt strong gt better chains and model diagnostics lt strong gt one package that turing and many others rely on heavily is lt a href quot https github com turinglang mcmcchains jl quot gt mcmcchains jl lt a gt a package designed to format store and analyze parameter samples generated during mcmc inference mcmcchains is currently showing its age a little and has many lt a href quot https github com turinglang mcmcchains jl issues 171 quot gt bad design choices lt a gt that need to be fixed alternatively a student could contstruct a far more lightweight chain system lt li gt lt li gt lt strong gt model comparison tools lt strong gt turing and its sattelite packages do not currently provide a comprehensive suite of model comparison tools a critical tool for the applied statistician a student who worked on this project would implement various model comparison tools like lt a href quot https mc stan org loo quot gt loo and waic lt a gt among others lt li gt lt li gt lt strong gt mle map tools lt strong gt lt a href quot https en wikipedia org wiki maximum likelihood estimation quot gt maximum likelihood estimates lt a gt mle and lt a href quot https en wikipedia org wiki maximum a posteriori estimation quot gt maximum a posteriori lt a gt map estimates can currently only be done by users through a lt a href quot https turing ml dev docs using turing advanced maximum a posteriori estimation quot gt clunky set of workarounds lt a gt a streamlined function like lt code class quot language plaintext highlighter rouge quot gt mle model lt code gt or lt code class quot language plaintext highlighter rouge quot gt map model lt code gt would be very useful for many of turing s users who want to see what the mle or map estimates look like and it may be valuable to allow for functionality that allows mcmc sampling to begin from the mle or map estimates students working on this project will work with optimization packages such as lt a href quot https github com julianlsolvers optim jl quot gt optim jl lt a gt to make mle and map estimation straightforward for turing models lt li gt lt li gt lt strong gt particle sampler improvements lt strong gt turing s development team has spent a lot of time and energy to make inference methods more modular but turing s particle samplers have not yet been modernized and spun off into a separate package two packages that resulted from this were lt a href quot https github com turinglang advancedhmc jl quot gt advancedhmc lt a gt for hamiltonian mcmc methods and lt a href quot https github com turinglang advancedmh jl quot gt advancedmh lt a gt for metropolis hastings style inference methods a student who worked on this project would become very familiar with turing s inference backend and with particle sampling methods this is a good project for people who love making things efficient and easily extendable lt li gt lt ul gt lt p gt other projects are welcome but we do strongly recommend discussing any potential projects with members of the turing team as they will end up mentoring gsoc students for the duration of the project lt p gt lt p gt we re looking forward to what people are interested in lt p gt wed 12 feb 2020 00 00 00 0000 v0 14 2 posts 2020 02 12 jsoc v0 14 2 posts 2020 02 12 jsoc turing s blog lt p gt all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like lt p gt lt ul gt lt li gt interesting things you can do with turing or interesting things we have seen others do lt li gt lt li gt development updates and major release announcements lt li gt lt li gt research updates lt li gt lt li gt explorations of turing s internals lt li gt lt li gt updates to turing s sattelite projects lt a href quot https github com turinglang advancedhmc jl quot gt advancedhmc jl lt a gt or lt a href quot https github com turinglang bijectors jl quot gt bijectors jl lt a gt lt li gt lt ul gt lt p gt stay tuned lt p gt sat 14 dec 2019 00 00 00 0000 v0 14 2 posts 2019 12 14 initial post v0 14 2 posts 2019 12 14 initial post", "title": ""},{"location": "/", "text": "turing jl bayesian inference with probabilistic programming intuitive turing models are easy to read and write models work the way you write them general purpose turing supports models with discrete parameters and stochastic control flow specify complex models quickly and easily modular turing is modular written fully in julia and can be modified to suit your needs high performance turing is fast hello world in turing linear gaussian model turing s modelling syntax allows you to specify a model quickly and easily straightforward models can be expressed in the same way as complex hierarchical models with stochastic control flow quick start model gdemo x y begin assumptions inversegamma 2 3 normal 0 sqrt observations x normal sqrt y normal sqrt end news feed google summer of code 2020 september 11 2020 replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 may 14 2020 google summer of code julia summer of code february 12 2020 turing s blog december 14 2019 news advanced markov chain monte carlo samplers turing provides hamiltonian monte carlo sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and gibbs sampling which combines particle mcmc hmc and many other mcmc algorithms samplers interoperable with deep learning libraries turing supports julia s flux package for automatic differentiation combine turing and flux to construct probabalistic variants of traditional machine learning models bayesian neural network tutorial community join the turing community to contribute learn and get your questions answered github report bugs request features discuss issues and more go to github turing jl discuss browse and join discussions on turing go to turing jl discuss slack discuss advanced topics request access here go to slack ecosystem explore a rich ecosystem of libraries tools and more to support development advancedhmc robust modular and efficient implementation of advanced hamiltonian monte carlo algorithms go to advancedhmc mcmcchains chain types and utility functions for mcmc simulations go to mcmcchains bijectors automatic transformations for constrained random variables go to bijectors", "title": "Turing.jl - Turing.jl"},{"location": "/news/", "text": "newssubscribe with rss to keep up with the latest news about turing google summer of code 2020 september 11 2020 as the 2020 google summer of code comes to a close the turing team thought it would be a good opportunity to reflect on the work that was done by our superb students this summer read more replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 may 14 2020 the turing jl team is currently exploring possibilities in an attempt to help with the ongoing sars cov 2 crisis as preparation for this and to get our feet wet we decided to perform a replication study of the imperial report 13 which attempts to estimate the real number of infections and impact of non pharmaceutical interventions on covid 19 in the report the inference was performed using the probabilistic programming language ppl stan we have explicated their model and inference in turing jl a julia based ppl we believe the results and analysis of our study are relevant for the public and for other researchers who are actively working on epidemiological models to that end our implementation and results are available here read more google summer of code julia summer of code february 12 2020 last year turing participated in the google summer of code gsoc through the julia language organization it was a fun time and the project was better for it turing plans to participate in the upcoming gsoc and we wanted to outline some potential projects and expectations we have for applicants read more turing s blog december 14 2019 all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like read more want to see more see the news archive", "title": "News"},{"location": "/search/search_index.json", "text": "config lang en prebuild index false separator s docs for page in site pages unless page excluded in search if added endif assign added false location page url text page content strip html strip newlines slugify ascii replace title page title assign added true endunless endfor for post in site posts unless page excluded in search if added endif assign added false location post url text post content strip html strip newlines slugify ascii replace title post title assign added true endunless endfor for doc in site docs unless doc excluded in search if added endif assign added false location doc url text doc content strip html strip newlines slugify ascii replace title doc title assign added true endunless endfor", "title": ""},{"location": "/sitemap.xml", "text": "now date y m d daily for section in site data toc site baseurl section url now date y m d daily endfor", "title": ""},{"location": "/robots.txt", "text": "sitemap sitemap xml absolute url", "title": ""},{"location": "/feed.xml", "text": "if page xsl endif jekyll site time date to xmlschema page url absolute url xml escape assign title site title default site name if page collection posts assign collection page collection capitalize assign title title append append collection endif if page category assign category page category capitalize assign title title append append category endif if title title smartify xml escape endif if site description site description xml escape endif if site author site author name default site author xml escape if site author email site author email xml escape endif if site author uri site author uri xml escape endif endif if page tags assign posts site tags page tags else assign posts site page collection endif if page category assign posts posts where category page category endif unless site show drafts assign posts posts where exp post post draft true endunless assign posts posts sort date reverse assign posts limit site feed posts limit default 10 for post in posts limit posts limit assign post title post title smartify strip html normalize whitespace xml escape post title post date date to xmlschema post last modified at default post date date to xmlschema post id absolute url xml escape assign excerpt only post feed excerpt only default site feed excerpt only unless excerpt only post content strip xml escape endunless assign post author post author default post authors 0 default site author assign post author site data authors post author default post author assign post author email post author email default nil assign post author uri post author uri default nil assign post author name post author name default post author post author name default xml escape if post author email post author email xml escape endif if post author uri post author uri xml escape endif if post category elsif post categories for category in post categories endfor endif for tag in post tags endfor if post excerpt and post excerpt empty post excerpt strip html normalize whitespace xml escape endif assign post image post image path default post image if post image unless post image contains assign post image post image absolute url endunless endif endfor", "title": ""},{"location": "/posts/2020-09-11-gsoc", "text": "as the 2020 google summer of code comes to a close the turing team thought it would be a good opportunity to reflect on the work that was done by our superb students this summer saranjeet kaur s project focused primarily on expanding nestedsamplers jl nestedsamplers jl now supports polychord style nested sampling natively which is an absolute delight saranjeet wrote about this here she also provided a good tutorial on how to use nestedsamplers jl here the nestedsamplers jl integration with turing is still on going integrating new samplers with turing is one of the more difficult tasks if you are interested to see the progress on this check out the relevant pull request arthur lui s project was to provide a much needed set of benchmarks of bayesian nonparametric models between turing and other ppls arthur s work spawned a github repository with good practices for benchmarking as well as three blog posts with some very cool statistics on turing s performance dirichlet process gaussian mixture model via the stick breaking construction in various ppls gaussian process regression model in various ppls gaussian process classification model in various pplsfinally sharan yalburgi a returning gsoc student completed an epic amount of work turing s growing suite of gaussian process tools in particular the github organization juliagaussianprocesses was founded and serves as an effort to build a robust gaussian process framework for the julia ecosystem the framework consists of multiple gp related julia packages kernelfunctions jl provides kernel functions for gps as well as efficient ad for these kernels kernelfunctions jl also supports multi output gps by providing necessary data abstractions and multi output kernels abstractgps jl defines gp abstractions and provides exact posteriors it provides support for induced points based gp posteriors and for efficient sequential online sparse gp updates gplikelihoods jl defines alternate likelihoods for non gaussian gps gpmlj jl provides a julia interface for gpflow a gp library written in python using tensorflow special thanks to our three gsoc students for this summer who all did excellent work additional thanks to google for supporting open source software development and the julia language", "title": "Google Summer of Code 2020"},{"location": "/posts/2020-05-04-Imperial-Report13-analysis", "text": "the turing jl team is currently exploring possibilities in an attempt to help with the ongoing sars cov 2 crisis as preparation for this and to get our feet wet we decided to perform a replication study of the imperial report 13 which attempts to estimate the real number of infections and impact of non pharmaceutical interventions on covid 19 in the report the inference was performed using the probabilistic programming language ppl stan we have explicated their model and inference in turing jl a julia based ppl we believe the results and analysis of our study are relevant for the public and for other researchers who are actively working on epidemiological models to that end our implementation and results are available here in summary we replicated the imperial covid 19 model using turing jl subsequently we compared the inference results between turing and stan and our comparison indicates that results are reproducible with two different implementations in particular we performed 4 sets of simulations using the imperial covid 19 model the resulting estimates of the expected real number of cases in contrast to the recorded number of cases the reproduction number r t and the expected number of deaths as a function of time and non pharmaceutical interventions npis for each simulation are shown below simulation a hypothetical simulation from the model without data prior predictive or non pharmaceutical interventions under the prior assumptions of the imperial covid 19 model there is a very wide range of epidemic progressions with expected cases from almost 0 to 100 of the population over time the black bar corresponds to the date of the last observation note that r t has a different time range than the other plots following the original report this shows the 100 days following the country specific epidemic start which is defined to be 31 days prior to the first date of 10 cumulative deaths while the other plots show the last 60 days simulation b future simulation with non pharmaceutical interventions kept in place posterior predictive after incorporating the observed infection data we can see a substantially more refined range of epidemic progression the reproduction rate estimate lies in the range of 3 5 5 6 before any intervention is introduced the dotted lines correspond to observations and the black bar corresponds to the date of the last observation simulation c future simulation with non pharmaceutical interventions removed now we see the hypothetical scenarios after incorporating infection data but with non pharmaceutical interventions removed this plot looks similar to simulation a but with a more rapid progression of the pandemic since the estimated reproduction rate is bigger than the prior assumptions the dotted lines correspond to observations and the black bar corresponds to the date of the last observation simulation d future simulation with when lockdown is lifted two weeks before the last observation predictive posterior as a result there is a clear rapid rebound of the reproduction rate comparing with simulation b we do not observe an immediate increase in the number of expected cases and deaths upon lifting lockdown but there is a significant difference in the number of cases and deaths in the last few days in the plot simulation d results in both greater number of cases and deaths as expected this demonstrates how the effects of lifting an intervention might not become apparent in the measurable variables e g deaths until several weeks later the dotted lines correspond to observations the black bar corresponds to the date of the last observation and the red bar indicates when lockdown was lifted overall simulation a shows the prior modelling assumptions and how these prior assumptions determine the predicted number of cases etc before seeing any data simulation b predicts the trend of the number of cases etc using estimated parameters and by keeping all the non pharmaceutical interventions in place simulation c shows the estimate in the case where none of the intervention measures are ever put in place simulation d shows the estimates in the case when the lockdown was lifted two weeks prior to the last observation while keeping all the other non pharmaceutical interventions in place we want to emphasise that we do not provide additional analysis of the imperial model yet nor are we aiming to make any claims about the validity or the implications of the model instead we refer to imperial report 13 for more details and analysis the purpose of this post is solely to add validation to the inference performed in the paper by obtaining the same results using a different probabilistic programming language ppl and by exploring whether or not turing jl can be useful for researchers working on these problems for our next steps we re looking at collaboration with other researchers and further developments of this and similar models there are some immediate directions to explore incoporation of more sources of data e g national mobility seasonal changes and behavior changes in individuals how the assumptions incorporated into the priors and their parameters change resulting posterior the current model does not directly include recovery as a possibility and assumes that if a person has been infected once then he she will be infectious until death number of recovered cases suffers from the same issues as the number of cases it cannot be directly observed but we can also deal with it in a similar manner as is done with number of cases and incorporate this into the model for a potential improvement this will result in a plethora of different models from which we can select the most realistic one using different model comparions techniques e g leave one out cross validation loo cv such model refinement can be potentially valuable given the high impact of this pandemic and the uncertainty and debates in the potential outcomes acknowledgement we would like to thank the julia community for creating such an excellent platform for scientific computing and for the continuous feedback that we have received we also thank researchers from computational and biological laboratory at cambridge university for their feedback on an early version of the post", "title": "Replication study: Estimating number of infections and impact of NPIs on COVID-19 in European countries (Imperial Report 13)"},{"location": "/posts/2020-02-12-jsoc", "text": "last year turing participated in the google summer of code gsoc through the julia language organization it was a fun time and the project was better for it turing plans to participate in the upcoming gsoc and we wanted to outline some potential projects and expectations we have for applicants if you are not aware google provides funds to students around the world to develop a project of their choice over the summer students receive funds from google and spend three months on any open source project the turing development team has prepared a list of possible projects that we have deemed valuable to the project and easy enough that it could feasibly be created in the three month limit this list is not exlusive if you have a good idea you can write it up in your proposal though it is recommend that you reach out to any of the turing team on julia s slack you can get an invite here or discourse messages on discourse should be posted to the probabilistic programming category we ll find you possible project ideas benchmarking turing s performance has been sporadically benchmarked against various other probabilistic programming languages e g turing stan pymc3 tensorflow prob but a systemic approach to studying where turing excels and where it falls short would be useful a gsoc student would implement identical models in many ppls and build tools to benchmark all ppls against one another nested sampling integration turing focuses on modularity in inference methods and the development team would like to see more inference methods particularly the popular nested sampling method a julia package nestedsamplers jl but it is not hooked up to turing and does not currently have a stable api a gsoc student would either integrate that package or construct their own nested sampling method and build it into turing automated function memoization by model annotation function memoization is a way to reduce costly function evaluation by caching the output when the same inputs are given turing s gibbs sampler often ends up rerunning expensive functions multiple times and it would be a significant performance improvement to allow turing s model compiler to automatically memoize functions where appropriate a student working on this project would become intimately familiar with turing s model compiler and build in various automated improvements making distributions gpu compatible julia s gpu tooling is generally quite good but currently turing is not able to reliably use gpus while sampling because distributions jl is not gpu compatible a student on this project would work with the turing developers and the distributions developers to allow the use of gpu parallelism where possible in turing static distributions small fixed size vectors and matrices are fairly common in turing models this means that sampling in turing can probably benefit from using statically sized vectors and matrices from staticarrays jl instead of the dynamic normal julia arrays beside the often superior performance of small static vectors and matrices static arrays are also automatically compatible with the gpu stack in julia currently the main obstacle to using staticarrays jl is that distributions in distributions jl are not compatible with staticarrays a gsoc student would adapt the multivariate and matrix variate distributions as well as the univariate distribution with vector parameters in distributions jl to make a spin off package called staticdistributions jl the student would then benchmark staticdistributions jl against distributions jl and showcase an example of using staticdistributions jl together with cuarrays jl and or cudanative jl for gpu acceleration gpnet extensions one of turing s sattelite packages gpnet is designed to provide a comprehensive suite of gaussian process tools see this issue for potential tasks there s a lot of interesting stuff going on with gps and this task in particular may have some creative freedom to it better chains and model diagnostics one package that turing and many others rely on heavily is mcmcchains jl a package designed to format store and analyze parameter samples generated during mcmc inference mcmcchains is currently showing its age a little and has many bad design choices that need to be fixed alternatively a student could contstruct a far more lightweight chain system model comparison tools turing and its sattelite packages do not currently provide a comprehensive suite of model comparison tools a critical tool for the applied statistician a student who worked on this project would implement various model comparison tools like loo and waic among others mle map tools maximum likelihood estimates mle and maximum a posteriori map estimates can currently only be done by users through a clunky set of workarounds a streamlined function like mle model or map model would be very useful for many of turing s users who want to see what the mle or map estimates look like and it may be valuable to allow for functionality that allows mcmc sampling to begin from the mle or map estimates students working on this project will work with optimization packages such as optim jl to make mle and map estimation straightforward for turing models particle sampler improvements turing s development team has spent a lot of time and energy to make inference methods more modular but turing s particle samplers have not yet been modernized and spun off into a separate package two packages that resulted from this were advancedhmc for hamiltonian mcmc methods and advancedmh for metropolis hastings style inference methods a student who worked on this project would become very familiar with turing s inference backend and with particle sampling methods this is a good project for people who love making things efficient and easily extendable other projects are welcome but we do strongly recommend discussing any potential projects with members of the turing team as they will end up mentoring gsoc students for the duration of the project we re looking forward to what people are interested in", "title": "Google Summer of Code/Julia Summer of Code"},{"location": "/posts/2019-12-14-initial-post", "text": "all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like interesting things you can do with turing or interesting things we have seen others do development updates and major release announcements research updates explorations of turing s internals updates to turing s sattelite projects advancedhmc jl or bijectors jl stay tuned", "title": "Turing's Blog"},{"location": "/docs/contributing/guide", "text": "contributingturing is an open source project if you feel that you have some relevant skills and are interested in contributing then please do get in touch you can contribute by opening issues on github or implementing things yourself and making a pull request we would also appreciate example models written using turing turing has a style guide it is not strictly necessary to review it before making a pull request but you may be asked to change portions of your code to conform with the style guide before it is merged how to contributegetting started fork this repository clone your fork on your local machine git clone https github com your username turing jl add a remote corresponding to this repository git remote add upstream https github com turinglang turing jl what can i do look at the issues page to find an outstanding issue for instance you could implement new features fix bugs or write example models git workflowfor more information on how the git workflow typically functions please see the github s introduction or julia s contribution guide", "title": "Contributing"},{"location": "/docs/contributing/style-guide", "text": "style guidethis style guide is adapted from invenia s style guide we would like to thank them for allowing us to access and use it please don t let not having read it stop you from contributing to turing no one will be annoyed if you open a pr whose style doesn t follow these conventions we will just help you correct it before it gets merged these conventions were originally written at invenia taking inspiration from a variety of sources including python s pep8 julia s notes for contributors and julia s style guide what follows is a mixture of a verbatim copy of invenia s original guide and some of our own modifications a word on consistencywhen adhering to this style it s important to realize that these are guidelines and not rules this is stated best in the pep8 a style guide is about consistency consistency with this style guide is important consistency within a project is more important consistency within one module or function is most important but most importantly know when to be inconsistent sometimes the style guide just doesn t apply when in doubt use your best judgment look at other examples and decide what looks best and don t hesitate to ask synopsisattempt to follow both the julia contribution guidelines the julia style guide and this guide when convention guidelines conflict this guide takes precedence known conflicts will be noted in this guide use 4 spaces per indentation level no tabs try to adhere to a 92 character line length limit use upper camel case convention for modules and types use lower case with underscores for method names note julia code likes to use lower case without underscores comments are good try to explain the intentions of the code use whitespace to make the code more readable no whitespace at the end of a line trailing whitespace avoid padding brackets with spaces ex int64 value preferred over int64 value editor configurationsublime text settingsif you are a user of sublime text we recommend that you have the following options in your julia syntax specific settings to modify these settings first open any julia file jl in sublime text then navigate to preferences gt settings more gt syntax specific user translate tabs to spaces true tab size 4 trim trailing white space on save true ensure newline at eof on save true rulers 92 vim settingsif you are a user of vim we recommend that you add the following options to your vimrc file set tabstop 4 sets tabstops to a width of four columns set softtabstop 4 determines the behaviour of tab and backspace keys with expandtab set shiftwidth 4 determines the results of gt gt lt lt and au filetype julia setlocal expandtab replaces tabs with spaces au filetype julia setlocal colorcolumn 93 highlights column 93 to help maintain the 92 character line limit by default vim seems to guess that jl files are written in lisp to ensure that vim recognizes julia files you can manually have it check for the jl extension but a better solution is to install julia vim which also includes proper syntax highlighting and a few cool other features atom settingsatom defaults preferred line length to 80 characters we want that at 92 for julia to change it go to atom gt preferences gt packages search for the language julia package and open the settings for it find preferred line length under julia grammar and change it to 92 code formattingfunction namingnames of functions should describe an action or property irrespective of the type of the argument the argument s type provides this information instead for example buyfood food should be buy food food names of functions should usually be limited to one or two lowercase words ideally write buyfood not buy food but if you are writing a function whose name is hard to read without underscores then please do use them method definitionsonly use short form function definitions when they fit on a single line yes foo x int64 abs x 3 no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data yes function foobar array data abstractarray t item t where t lt int64 return t abs x abs item 3 for x in array data endwhen using long form functions always use the return keyword yes function fnc x result zero x result fna x return resultend no function fnc x result zero x result fna x end yes function foo x y return new x y end no function foo x y new x y endfunctions definitions with parameter lines which exceed 92 characters should separate each parameter by a newline and indent by one level yes function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend ok function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeendkeyword argumentswhen calling a function always separate your keyword arguments from your positional arguments with a semicolon this avoids mistakes in ambiguous cases such as splatting a dict yes xy foo x y 3 no xy foo x y 3 whitespaceavoid extraneous whitespace in the following situations immediately inside parentheses square brackets or braces yes spam ham 1 eggs no spam ham 1 eggs immediately before a comma or semicolon yes if x 4 show x y x y y x endno if x 4 show x y x y y x end when using ranges unless additional operators are used yes ham 1 9 ham 1 3 9 ham 1 3 end no ham 1 9 ham 1 3 9 yes ham lower upper ham lower step upper yes ham lower offset upper offset yes ham lower offset upper offset no ham lower offset upper offset more than one space around an assignment or other operator to align it with another yes x 1y 2long variable 3 no x 1y 2long variable 3 when using parametric types yes f a abstractarray t n where t lt real n g a abstractarray lt real n where n no f a abstractarray t n where t lt real n g a abstractarray lt real n where n always surround these binary operators with a single space on either side assignment updating operators etc numeric comparisons operators lt gt etc note that this guideline does not apply when performing assignment in method definitions yes i i 1no i i 1yes submitted 1no submitted 1yes x 2 lt yno x 2 lt y assignments using expanded array tuple or function notation should have the first open bracket on the same line assignment operator and the closing bracket should match the indentation level of the assignment alternatively you can perform assignments on a single line when they are short yes arr 1 2 3 arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 arr 1 2 3 arr 1 2 3 nested array or tuples that are in expanded notation should have the opening and closing brackets at the same indentation level yes x 1 2 3 hello world a b c no y 1 2 3 hello world z 1 2 3 hello world always include the trailing comma when working with expanded arrays tuples or functions notation this allows future edits to easily move elements around or add additional elements the trailing comma should be excluded when the notation is only on a single line yes arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 result function arg1 arg2 arr 1 2 3 triple quotes use the indentation of the lowest indented line excluding the opening triple quote this means the closing triple quote should be aligned to least indented line in the string triple backticks should also follow this style even though the indentation does not matter for them yes str hello world str hello world cmd program flag value parameter no str hello world commentscomments should be used to state the intended behaviour of code this is especially important when the code is doing something clever that may not be obvious upon first inspection avoid writing comments that state exactly what the code obviously does yes x x 1 compensate for border no x x 1 increment xcomments that contradict the code are much worse than no comments always make a priority of keeping the comments up to date with code changes comments should be complete sentences if a comment is a phrase or sentence its first word should be capitalized unless it is an identifier that begins with a lower case letter never alter the case of identifiers if a comment is short the period at the end can be omitted block comments generally consist of one or more paragraphs built out of complete sentences and each sentence should end in a period comments should be separated by at least two spaces from the expression and have a single space after the when referencing julia in documentation note that julia refers to the programming language while julia typically in backticks e g julia refers to the executable a commmentcode another commentmore codetododocumentationit is recommended that most modules types and functions should have docstrings that being said only exported functions are required to be documented avoid documenting methods like as the built in docstring for the function already covers the details well try to document a function and not individual methods where possible as typically all methods will have similar docstrings if you are adding a method to a function which was defined in base or another package only add a docstring if the behaviour of your function deviates from the existing docstring docstrings are written in markdown and should be concise docstring lines should be wrapped at 92 characters bar x y compute the bar index between x and y if y is missing compute the bar index betweenall pairs of columns of x function bar x y when types or methods have lots of parameters it may not be feasible to write a concise docstring in these cases it is recommended you use the templates below note if a section doesn t apply or is overly verbose for example throws if your function doesn t throw an exception it can be excluded it is recommended that you have a blank line between the headings and the content when the content is of sufficient length try to be consistent within a docstring whether you use this additional whitespace note that the additional space is only for reading raw markdown and does not effect the rendered version type template should be skipped if is redundant with the constructor s docstring myarray t n my super awesome array wrapper fields data abstractarray t n stores the array being wrapped metadata dict stores metadata about the array struct myarray t n lt abstractarray t n data abstractarray t n metadata dictendfunction template only required for exported functions mysearch array myarray t val t verbose true where t gt intsearches the array for the val for some reason we don t want to use julia sbuiltin search arguments array myarray t the array to search val t the value to search for keywords verbose bool true print out progress details returns int the index where val is located in the array throws notfounderror i guess we could throw an error if val isn t found function mysearch array abstractarray t val t where t endif your method contains lots of arguments or keywords you may want to exclude them from the method signature on the first line and instead use args and or kwargs manager args kwargs gt managera cluster manager which spawns workers arguments min workers integer the minimum number of workers to spawn or an exception is thrown max workers integer the requested number of worker to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endfeel free to document multiple methods for a function within the same docstring be careful to only do this for functions you have defined manager max workers kwargs manager min workers max workers kwargs manager min workers max workers kwargs a cluster manager which spawns workers arguments min workers int the minimum number of workers to spawn or an exception is thrown max workers int the number of requested workers to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endif the documentation for bullet point exceeds 92 characters the line should be wrapped and slightly indented avoid aligning the text to the keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance for additional details on documenting in julia see the official documentation test formattingtestsetsjulia provides test sets which allows developers to group tests into logical groupings test sets can be nested and ideally packages should only have a single root test set it is recommended that the runtests jl file contains the root test set which contains the remainder of the tests testset pkgextreme begin include arithmetic jl include utils jl endthe file structure of the test folder should mirror that of the src folder every file in src should have a complementary file in the test folder containing tests relevant to that file s contents comparisonsmost tests are written in the form test x y since the function doesn t take types into account tests like the following are valid test 1 0 1 avoid adding visual noise into test comparisons yes test value 0 no test value 0 0in cases where you are checking the numerical validity of a model s parameter estimates please use the check numerical function found in test test utils numerical tests jl this function will evaluate a model s parameter estimates using tolerance levels atol and rtol testing will only be performed if you are running the test suite locally or if travis is executing the numerical testing stage here is an example of usage check that m and s are plus or minus one from 1 5 and 2 2 respectively check numerical chain m s 1 5 2 2 atol 1 0 checks the estimates for a default gdemo model using values 1 5 and 2 0 check gdemo chain atol 0 1 checks the estimates for a default mog model check mogtest default chain atol 0 1", "title": "Style Guide"},{"location": "/docs/for-developers/compiler", "text": "in this section the current design of turing s model compiler is described which enables turing to perform various types of bayesian inference without changing the model definition the compiler is essentially just a macro that rewrites the user s model definition to a function that generates a model struct that julia s dispatch can operate on and that julia s compiler can successfully do type inference on for efficient machine code generation overviewthe following terminology will be used in this section d observed data variables conditioned upon in the posterior p parameter variables distributed according to the prior distributions these will also be referred to as random variables model a fully defined probabilistic model with input dataturing s model macro rewrites the user provided function definition such that it can be used to instantiate a model by passing in the observed data d the following are the main jobs of the model macro parse and lines e g y normal c x 1 0 figure out if a variable belongs to the data d and or to the parameters p enable the handling of missing data variables in d when defining a model and treating them as parameter variables in p instead enable the tracking of random variables using the data structures varname and varinfo change lines with a variable in p on the lhs to a call to tilde assume or dot tilde assume change lines with a variable in d on the lhs to a call to tilde observe or dot tilde observe enable type stable automatic differentiation of the model using type parametersthe modela model model is a callable struct that one can sample from by calling model model rng varinfo sampler context where rng is a random number generator default random global rng varinfo is a data structure that stores information about the random variables default dynamicppl varinfo sampler is a sampling algorithm default dynamicppl samplefromprior and context is a sampling context that can e g modify how the log probability is accumulated default dynamicppl defaultcontext sampling resets the log joint probability of varinfo and increases the evaluation counter of sampler if context is a likelihoodcontext only the log likelihood will be accumulated with the defaultcontext the log joint probability of p and d is accumulated the model struct contains the three internal fields f args and defaults when model model is called then the internal function model f is called as model f rng varinfo sampler context model args for multithreaded sampling instead of varinfo a threadsafe wrapper is passed to model f the positional and keyword arguments that were passed to the user defined model function when the model was created are saved as a namedtuple in model args the default values of the positional and keyword arguments of the user defined model functions if any are saved as a namedtuple in model defaults they are used for constructing model instances with different arguments by the logprob and prob string macros examplelet s take the following model as an example model function gauss x missing y 1 0 type tv vector float64 where tv lt abstractvector if x missing x tv undef 3 end p tv undef 2 p 1 inversegamma 2 3 p 2 normal 0 1 0 x 1 2 normal p 2 sqrt p 1 x 3 normal y normal p 2 sqrt p 1 endthe above call of the model macro defines the function gauss with positional arguments x y and type tv rewritten in such a way that every call of it returns a model model note that only the function body is modified by the model macro and the function signature is left untouched it is also possible to implement models with keyword arguments such as model function gauss type tv vector float64 x missing y 1 0 where tv lt abstractvector endthis would allow us to generate a model by calling gauss x rand 3 if an argument has a default value missing it is treated as a random variable for variables which require an intialization because we need to loop or broadcast over its elements such as x above the following needs to be done if x missing x endnote that since gauss behaves like a regular function it is possible to define additional dispatches in a second step as well for instance we could achieve the same behaviour by model function gauss x y 1 0 type tv vector float64 where tv lt abstractvector p tv undef 2 endfunction gauss missing y 1 0 type tv vector float64 where tv lt abstractvector return gauss tv undef 3 y tv endif x is sampled as a whole from a distribution and not indexed e g x normal or x mvnormal there is no need to initialize it in an if block step 1 break up the model definitionfirst the model macro breaks up the user provided function definition using dynamicppl build model info this function returns a dictionary consisting of allargs exprs the expressions of the positional and keyword arguments without default values allargs syms the names of the positional and keyword arguments e g x y tv above allargs namedtuple an expression that constructs a namedtuple of the positional and keyword arguments e g x x y y tv tv above defaults namedtuple an expression that constructs a namedtuple of the default positional and keyword arguments if any e g x missing y 1 tv vector float64 above modeldef a dictionary with the name arguments and function body of the model definition as returned by macrotools splitdef step 2 generate the body of the internal model functionin a second step dynamicppl generate mainbody generates the main part of the transformed function body using the user provided function body and the provided function arguments without default values for figuring out if a variable denotes an observation or a random variable hereby the function dynamicppl generate tilde replaces the l r lines in the model and the function dynamicppl generate dot tilde replaces the l r and l r lines in the model in the above example p 1 inversegamma 2 3 is replaced with something similar to repl 25 6 begin var tmpright 323 inversegamma 2 3 var tmpright 323 isa union distribution abstractvector lt distribution throw argumenterror right hand side of a must be subtype of distribution or a vector of distributions var vn 325 dynamicppl varname p 1 var inds 326 1 p 1 dynamicppl tilde assume rng context sampler var tmpright 323 var vn 325 var inds 326 varinfo endhere the first line is a so called line number node that enables more helpful error messages by providing users with the exact location of the error in their model definition then the right hand side rhs of the is assigned to a variable with an automatically generated name we check that the rhs is a distribution or an array of distributions otherwise an error is thrown next we extract a compact representation of the variable with its name and index or indices finally the expression is replaced with a call to dynamicppl tilde assume since the compiler figured out that p 1 is a random variable using the following heuristic if the symbol on the lhs of p in this case is not among the arguments to the model x y t in this case it is a random variable if the symbol on the lhs of p in this case is among the arguments to the model but has a value of missing it is a random variable if the value of the lhs of p 1 in this case is missing then it is a random variable otherwise it is treated as an observation the dynamicppl tilde assume function takes care of sampling the random variable if needed and updating its value and the accumulated log joint probability in the varinfo object if l r is an observation dynamicppl tilde observe is called with the same arguments except the random number generator rng since observations are never sampled a similar transformation is performed for expressions of the form l r and l r for instance x 1 2 normal p 2 sqrt p 1 is replaced with repl 25 8 begin var tmpright 331 normal p 2 sqrt p 1 var tmpright 331 isa union distribution abstractvector lt distribution throw argumenterror right hand side of a must be subtype of distribution or a vector of distributions var vn 333 dynamicppl varname x 1 2 var inds 334 1 2 var isassumption 335 begin let var vn 336 dynamicppl varname x 1 2 if dynamicppl inargnames var vn 336 model dynamicppl inmissings var vn 336 model true else x 1 2 missing end end end if var isassumption 335 x 1 2 dynamicppl dot tilde assume rng context sampler var tmpright 331 x 1 2 var vn 333 var inds 334 varinfo else dynamicppl dot tilde observe context sampler var tmpright 331 x 1 2 var vn 333 var inds 334 varinfo endendthe main difference in the expanded code between l r and l r is that the former doesn t assume l to be defined it can be a new julia variable in the scope while the latter assumes l already exists moreover dynamicppl dot tilde assume and dynamicppl dot tilde observe are called instead of dynamicppl tilde assume and dynamicppl tilde observe step 3 replace the user provided function bodyfinally we replace the user provided function body using dynamicppl build output this function uses macrotools combinedef to reassemble the user provided function with a new function body in the modified function body an anonymous function is created whose function body was generated in step 2 above and whose arguments are a random number generator rng a model model a datastructure varinfo a sampler sampler a sampling context context and all positional and keyword arguments of the user provided model function as positional argumentswithout any default values finally in the new function body a model model with this anonymous function as internal function is returned varnamein order to track random variables in the sampling process turing uses the struct varname sym which acts as a random variable identifier generated at runtime the varname of a random variable is generated from the expression on the lhs of a statement when the symbol on the lhs is in p every vn varname sym has a symbol sym which is the symbol of the julia variable in the model that the random variable belongs to for example x 1 normal will generate an instance of varname x assuming x is in p every vn varname also has a field indexing which stores the indices requires to access the random variable from the julia variable indicated by sym for example x 1 normal will generate a vn varname x with vn indexing 1 varname also supports hierarchical arrays and range indexing some more examples x 1 normal will generate a varname x with indexing 1 x 1 mvnormal zeros 2 will generate a varname x with indexing colon 1 x 1 2 normal will generate a varname x with indexing colon 1 2 varinfooverviewvarinfo is the data structure in turing that facilitates tracking random variables and certain metadata about them that are required for sampling for instance the distribution of every random variable is stored in varinfo because we need to know the support of every random variable when sampling using hmc for example random variables whose distributions have a constrained support are transformed using a bijector from bijectors jl so that the sampling happens in the unconstrained space different samplers require different metadata about the random variables the definition of varinfo in turing is struct varinfo tmeta tlogp lt abstractvarinfo metadata tmeta logp base refvalue tlogp num produce base refvalue int endbased on the type of metadata the varinfo is either aliased untypedvarinfo or typedvarinfo metadata can be either a subtype of the union type metadata or a namedtuple of multiple such subtypes let vi be an instance of varinfo if vi isa varinfo lt metadata then it is called an untypedvarinfo if vi isa varinfo lt namedtuple then vi metadata would be a namedtuple mapping each symbol in p to an instance of metadata vi would then be called a typedvarinfo the other fields of varinfo include logp which is used to accumulate the log probability or log probability density of the variables in p and d num produce keeps track of how many observations have been made in the model so far this is incremented when running a statement when the symbol on the lhs is in d metadatathe metadata struct stores some metadata about the random variables sampled this helps query certain information about a variable such as its distribution which samplers sample this variable its value and whether this value is transformed to real space or not let md be an instance of metadata md vns is the vector of all varname instances let vn be an arbitrary element of md vns md idcs is the dictionary that maps each varname instance to its index inmd vns md ranges md dists md orders and md flags md vns md idcs vn vn md dists md idcs vn is the distribution of vn md gids md idcs vn is the set of algorithms used to sample vn this is used inthe gibbs sampling process md orders md idcs vn is the number of observe statements before vn is sampled md ranges md idcs vn is the index range of vn in md vals md vals md ranges md idcs vn is the linearized vector of values of corresponding to vn md flags is a dictionary of true false flags md flags flag md idcs vn is thevalue of flag corresponding to vn note that in order to make md metadata type stable all the md vns must have the same symbol and distribution type however one can have a single julia variable e g x that is a matrix or a hierarchical array sampled in partitions e g x 1 mvnormal zeros 2 1 0 x 2 mvnormal ones 2 1 0 the symbol x can still be managed by a single md metadata without hurting the type stability since all the distributions on the rhs of are of the same type however in turing models one cannot have this restriction so we must use a type unstable metadata if we want to use one metadata instance for the whole model this is what untypedvarinfo does a type unstable metadata will still work but will have inferior performance to strike a balance between flexibility and performance when constructing the spl sampler instance the model is first run by sampling the parameters in p from their priors using an untypedvarinfo i e a type unstable metadata is used for all the variables then once all the symbols and distribution types have been identified a vi typedvarinfo is constructed where vi metadata is a namedtuple mapping each symbol in p to a specialized instance of metadata so as long as each symbol in p is sampled from only one type of distributions vi typedvarinfo will have fully concretely typed fields which brings out the peak performance of julia", "title": "Turing Compiler Design"},{"location": "/docs/for-developers/how_turing_implements_abstractmcmc", "text": "how turing implements abstractmcmcprerequisite interface guide introductionconsider the following turing code block model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endmod gdemo 1 5 2 alg is n samples 1000chn sample mod alg n samples the function sample is part of the abstractmcmc interface as explained in the interface guide building a a sampling method that can be used by sample consists in overloading the structs and functions in abstractmcmc the interface guide also gives a standalone example of their implementation advancedmh jl turing sampling methods most of which are written here also implement abstractmcmc turing defines a particular architecture for abstractmcmc implementations that enables working with models defined by the model macro and uses dynamicppl as a backend the goal of this page is to describe this architecture and how you would go about implementing your own sampling method in turing using importance sampling as an example i don t go into all the details for instance i don t address selectors or parallelism first we explain how importance sampling works in the abstract consider the model defined in the first code block mathematically it can be written begin align s amp sim text inversegamma 2 3 m amp sim text normal 0 sqrt s x amp sim text normal m sqrt s y amp sim text normal m sqrt s end align the latent variables are s and m the observed variables are x and y the model joint distribution p s m x y decomposes into the prior p s m and the likelihood p x y mid s m since x 1 5 and y 2 are observed the goal is to infer the posterior distribution p s m mid x y importance sampling produces independent samples s i m i from the prior distribution it also outputs unnormalized weights w i frac p x y s i m i p s i m i p x y mid s i m i such that the empirical distribution frac 1 n sum limits i 1 n frac w i sum limits j 1 n w j delta s i m i is a good approximation of the posterior 1 define a samplerrecall the last line of the above code block chn sample mod alg n samples here sample takes as arguments a model mod an algorithm alg and a number of samples n samples and returns an instance chn of chains which can be analysed using the functions in mcmcchains modelsto define a model you declare a joint distribution on variables in the model macro and specify which variables are observed and which should be inferred as well as the value of the observed variables thus when implementing importance sampling mod gdemo 1 5 2 creates an instance mod of the struct model which corresponds to the observations of a value of 1 5 for x and a value of 2 for y this is all handled by dynamicppl more specifically here i will return to how models are used to inform sampling algorithms below algorithmsan algorithm is just a sampling method in turing it is a subtype of the abstract type inferencealgorithm defining an algorithm may require specifying a few high level parameters for example hamiltonian monte carlo may be too vague but hamiltonian monte carlo with 10 leapfrog steps per proposal and a stepsize of 0 01 is an algorithm metropolis hastings may be too vague but metropolis hastings with proposal distribution p is an algorithm epsilon thusstepsize 0 01l 10alg hmc stepsize l defines a hamiltonian monte carlo algorithm an instance of hmc which is a subtype of inferencealgorithm in the case of importance sampling there is no need to specify additional parameters alg is defines an importance sampling algorithm an instance of is which is a subtype of inferencealgorithm when creating your own turing sampling method you must therefore build a subtype of inferencealgorithm corresponding to your method samplerssamplers are not the same as algorithms an algorithm is a generic sampling method a sampler is an object that stores information about how algorithm and model interact during sampling and is modified as sampling progresses the sampler struct is defined in dynamicppl turing implements abstractmcmc s abstractsampler with the sampler struct defined in dynamicppl the most important attributes of an instance spl of sampler are spl alg the sampling method used an instance of a subtype of inferencealgorithm spl state information about the sampling process see belowwhen you call sample mod alg n samples turing first uses model and alg to build an instance spl of sampler then calls the native abstractmcmc function sample mod spl n samples when you define your own turing sampling method you must therefore build a sampler constructor that uses a model and an algorithm to initialize an instance of sampler for importance sampling function sampler alg is model model s selector info dict symbol any state isstate model return sampler alg info s state end a state struct implementing abstractsamplerstate corresponding to your method we cover this in the following paragraph statesthe vi field contains all the important information about sampling first and foremost the values of all the samples but also the distributions from which they are sampled the names of model parameters and other metadata as we will see below many important steps during sampling correspond to queries or updates to spl state vi by default you can use samplerstate a concrete type defined in inference inference jl which extends abstractsamplerstate and has no field except for vi mutable struct samplerstate vitype lt varinfo lt abstractsamplerstate vi vitypeendwhen doing importance sampling we care not only about the values of the samples but also their weights we will see below that the weight of each sample is also added to spl state vi moreover the average frac 1 n sum limits j 1 n w i frac 1 n sum limits j 1 n p x y mid s i m i of the sample weights is a particularly important quantity it is used to normalize the empirical approximation of the posterior distribution its logarithm is the importance sampling estimate of the log evidence log p x y to avoid having to compute it over and over again is jldefines an is specific concrete type isstate for sampler states with an additional field final logevidence containing log left frac 1 n sum limits j 1 n w i right mutable struct isstate v lt varinfo f lt abstractfloat lt abstractsamplerstate vi v final logevidence fend additional constructorisstate model model isstate varinfo model 0 0 the following diagram summarizes the hierarchy presented above 2 overload the functions used inside mcmcsamplea lot of the things here are method specific however turing also has some functions that make it easier for you to implement these functions for examples transitionsabstractmcmc stores information corresponding to each individual sample in objects called transition but does not specify what the structure of these objects could be you could decide to implement a type mytransition for transitions corresponding to the specifics of your methods however there are many situations in which the only information you need for each sample is its value theta log of the joint probability of the observed data and this sample lpinference jl defines a struct transition which corresponds to this default situationstruct transition t f lt abstractfloat t lp fendit also contains a constructor that builds an instance of transition from an instance spl of sampler theta is spl state vi converted to a namedtuple and lp is getlogp spl state vi is jl uses this default constructor at the end of the step function here how sample worksa crude summary which ignores things like parallelism is the following sample calls mcmcsample which calls sample init to set things up step repeatedly to produce multiple new transitions sample end to perform operations once all samples have been obtained bundle samples to convert a vector of transitions into a more palatable type for instance a chain you can of course implement all of these functions but abstractmcmc as well as turing also provide default implementations for simple cases for instance importance sampling uses the default implementations of sample init and bundle samples which is why you don t see code for them inside is jl 3 overload assume and observethe functions mentioned above such as sample init step etc must of course use information about the model in order to generate samples in particular these functions may need samples from distributions defined in the model or to evaluate the density of these distributions at some values of the corresponding parameters or observations for an example of the former consider importance sampling as defined in is jl this implementation of importance sampling uses the model prior distribution as a proposal distribution and therefore requires samples from the prior distribution of the model another example is approximate bayesian computation which requires multiple samples from the model prior and likelihood distributions in order to generate a single sample an example of the latter is the metropolis hastings algorithm at every step of sampling from a target posterior p theta mid x text obs in order to compute the acceptance ratio you need to evaluate the model joint density p theta text prop x text obs with theta text prop a sample from the proposal and x text obs the observed data this begs the question how can these functions access model information during sampling recall that the model is stored as an instance m of model one of the attributes of m is the model evaluation function m f which is built by compiling the model macro executing f runs the tilde statements of the model in order and adds model information to the sampler the instance of sampler that stores information about the ongoing sampling process at each step see here for more information about how the model macro is compiled the dynamicppl functions assume and observe determine what kind of information to add to the sampler for every tilde statement consider an instance m of model and a sampler spl with associated varinfo vi spl state vi at some point during the sampling process an abstractmcmc function such as step calls m vi which calls the model evaluation function m f vi for every tilde statement in the model macro m f vi returns model related information samples value of the model density etc and adds it to vi how does it do that recall that the code for m f vi is automatically generated by compilation of the model macro for every tilde statement in the model declaration this code contains a call to assume vi if the variable on the lhs of the tilde is a model parameter to infer and observe vi if the variable on the lhs of the tilde is an observation in the file corresponding to your sampling method ie in turing jl src inference lt your method gt jl you have overloaded assume and observe so that they can modify vi to include the information and samples that you care about at a minimum assume and observe return the log density lp of the sample or observation the model evaluation function then immediately calls acclogp vi lp which adds lp to the value of the log joint density stored in vi here s what assume looks like for importance sampling function dynamicppl assume rng spl sampler lt is dist distribution vn varname vi r rand rng dist push vi vn r dist spl return r 0endthe function first generates a sample r from the distribution dist the right hand side of the tilde statement it then adds r to vi and returns r and 0 the observe function is even simpler function dynamicppl observe spl sampler lt is dist distribution value vi return logpdf dist value endit simply returns the density in the discrete case the probability of the observed value under the distribution dist 4 summary importance sampling step by stepwe focus on the abstractmcmc functions that are overriden in is jl and executed inside mcmcsample step which is called n samples times and sample end which is executed once after those n samples iterations during the i th iteration step does 3 things empty spl state vi remove information about the previous sample from the sampler s varinfo model rng spl state vi spl call the model evaluation function calls to assume add the samples from the prior s i and m i to spl state vi calls to both assume or observe are followed by the line acclogp vi lp where lp is an output of assume and observe lp is set to 0 after assume and to the value of the density at the observation after observe when all the tilde statements have been covered spl state vi logp is the sum of the lp ie the likelihood log p x y mid s i m i log p x mid s i m i log p y mid s i m i of the observations given the latent variable samples s i and m i return transition spl build a transition from the sampler and return that transition the transition s vi field is simply spl state vi the lp field contains the likelihood spl state vi logp when the n samples iterations are completed sample end fills the final logevidence field of spl state it simply takes the logarithm of the average of the sample weights using the log weights for numerical stability", "title": "How Turing implements AbstractMCMC"},{"location": "/docs/for-developers/interface", "text": "the sampling interfaceturing implements a sampling interface hosted at abstractmcmc that is intended to provide a common framework for markov chain monte carlo samplers the interface presents several structures and functions that one needs to overload in order to implement an interface compatible sampler this guide will demonstrate how to implement the interface without turing interface overviewany implementation of an inference method that uses the abstractmcmc interface should implement a subset of the following types and functions a subtype of abstractsampler defined as a mutable struct containing state information or sampler parameters a function sample init which performs any necessary set up default do not perform any set up a function step which returns a transition that represents a single draw from the sampler a function transitions init which returns a container for the transitions obtained from the sampler default return a vector t of length n where t is the type of the transition obtained in the first step and n is the number of requested samples a function transitions save which saves transitions to the container default save the transition of iteration i at position i in the vector of transitions a function sample end which handles any sampler wrap up default do not perform any wrap up a function bundle samples which accepts the container of transitions and returns a collection of samples default return the vector of transitions the interface methods with exclamation points are those that are intended to allow for state mutation any mutating function is meant to allow mutation where needed you might use sample init to run some kind of sampler preparation before sampling begins this could mutate a sampler s state step might mutate a sampler flag after each sample sample end contains any wrap up you might need to do if you were sampling in a transformed space this might be where you convert everything back to a constrained space why do you have an interface the motivation for the interface is to allow julia s fantastic probabilistic programming language community to have a set of standards and common implementations so we can all thrive together markov chain monte carlo methods tend to have a very similar framework to one another and so a common interface should help more great inference methods built in single purpose packages to experience more use among the community implementing metropolis hastings without turingmetropolis hastings is often the first sampling method that people are exposed to it is a very straightforward algorithm and is accordingly the easiest to implement so it makes for a good example in this section you will learn how to use the types and functions listed above to implement the metropolis hastings sampler using the mcmc interface the full code for this implementation is housed in advancedmh jl importslet s begin by importing the relevant libraries we ll import abstracmcmc which contains the interface framework we ll fill out we also need distributions and random import the relevant libraries import abstractmcmcusing distributionsusing randoman interface extension like the one we re writing right now typically requires that you overload or implement several functions specifically you should import the functions you intend to overload this next code block accomplishes that from distributions we need sampleable variateform and valuesupport three abstract types that define a distribution models in the interface are assumed to be subtypes of sampleable variateform valuesupport in this section our model is going be be extremely simple so we will not end up using these except to make sure that the inference functions are dispatching correctly samplerlet s begin our sampler definition by defining a sampler called metropolishastings which is a subtype of abstractsampler correct typing is very important for proper interface implementation if you are missing a subtype your method may not be dispatched to when you call sample define a sampler type struct metropolishastings t d lt abstractmcmc abstractsampler init t proposal dend default constructors metropolishastings init real metropolishastings init normal 0 1 metropolishastings init vector lt real metropolishastings init mvnormal length init 1 above we have defined a sampler that stores the initial parameterization of the prior and a distribution object from which proposals are drawn you can have a struct that has no fields and simply use it for dispatching onto the relevant functions or you can store a large amount of state information in your sampler the general intuition for what to store in your sampler struct is that anything you may need to perform inference between samples but you don t want to store in a transition should go into the sampler struct it s the only way you can carry non sample related state information between step calls modelnext we need to have a model of some kind a model is a struct that s a subtype of abstractmodel that contains whatever information is necessary to perform inference on your problem in our case we want to know the mean and variance parameters for a standard normal distribution so we can keep our model to the log density of a normal note that we only have to do this because we are not yet integrating the sampler with turing turing has a very sophisticated modelling engine that removes the need to define custom model structs define a model type stores the log density function struct densitymodel f lt function lt abstractmcmc abstractmodel fendtransitionthe next step is to define some transition which we will return from each step call we ll keep it simple by just defining a wrapper struct that contains the parameter draws and the log density of that draw create a very basic transition type only stores the parameter draws and the log probability of the draw struct transition t l t lp lend store the new draw and its log density transition model densitymodel transition model transition can now store any type of parameter whether it s a vector of draws from multiple parameters or a single univariate draw metropolis hastingsnow it s time to get into the actual inference we ve defined all of the core pieces we need but we need to implement the step function which actually performs inference as a refresher metropolis hastings implements a very basic algorithm pick some initial state theta 0 for t in 1 n do a generate a proposal parameterization t sim q theta t mid theta t 1 b calculate the acceptance probability alpha text min big 1 frac pi t pi theta t 1 frac q t 1 mid t q t mid t 1 big c if u le where u sim 0 1 then theta t theta t otherwise theta t theta t 1 of course it s much easier to do this in the log space so the acceptance probability is more commonly written as alpha min big log pi t log pi t 1 log q t 1 mid t log q t mid t 1 0 big in interface terms we should do the following make a new transition containing a proposed sample calculate the acceptance probability if we accept return the new transition otherwise return the old one stepsthe step function is the function that performs the bulk of your inference in our case we will implement two step functions one for the very first iteration and one for every subsequent iteration define the first step function which is called at the beginning of sampling return the initial parameter used to define the sampler function abstractmcmc step rng abstractrng model densitymodel spl metropolishastings n integer nothing kwargs return transition model spl init endthe first step function just packages up the initial parameterization inside the sampler and returns it we implicity accept the very first parameterization the other step function performs the usual steps from metropolis hastings included are several helper functions proposal and q which are designed to replicate the functions in the pseudocode above proposal generates a new proposal in the form of a transition which can be univariate if the value passed in is univariate or it can be multivariate if the transition given is multivariate proposals use a basic normal or mvnormal proposal distribution q returns the log density of one parameterization conditional on another according to the proposal distribution step generates a new proposal checks the acceptance probability and then returns either the previous transition or the proposed transition define a function that makes a basic proposal depending on a univariate parameterization or a multivariate parameterization propose spl metropolishastings model densitymodel real transition model rand spl proposal propose spl metropolishastings model densitymodel vector lt real transition model rand spl proposal propose spl metropolishastings model densitymodel t transition propose spl model t calculates the probability q cond using the proposal distribution spl proposal q spl metropolishastings real cond real logpdf spl proposal cond q spl metropolishastings vector lt real cond vector lt real logpdf spl proposal cond q spl metropolishastings t1 transition t2 transition q spl t1 t2 calculate the density of the model given some parameterization model densitymodel model model densitymodel t transition t lp define the other step function returns a transition containing either a new proposal if accepted or the previous proposal if not accepted function abstractmcmc step rng abstractrng model densitymodel spl metropolishastings integer prev transition kwargs generate a new proposal propose spl model prev calculate the log acceptance probability model model prev q spl prev q spl prev decide whether to return the previous or the new one if log rand rng lt min 0 0 return else return prev endendchainsin the default implementation sample just returns a vector of all transitions if instead you would like to obtain a chains object e g to simplify downstream analysis you have to implement the bundle samples function as well it accepts the vector of transitions and returns a collection of samples fortunately our transition is incredibly simple and we only need to build a little bit of functionality to accept custom parameter names passed in by the user a basic chains constructor that works with the transition struct we defined function abstractmcmc bundle samples rng abstractrng densitymodel s metropolishastings n integer ts vector lt transition chain type type any param names missing kwargs turn all the transitions into a vector of vectors vals copy reduce hcat vcat t t lp for t in ts check if we received any parameter names if ismissing param names param names parameter i for i in 1 length first vals 1 end add the log density field to the parameter names push param names lp bundle everything up and return a chains struct return chains vals param names internals lp endall done you can even implement different output formats by implementing bundle samples for different chain types which can be provided as keyword argument to sample as default sample uses chain type any testing the implementationnow that we have all the pieces we should test the implementation by defining a model to calculate the mean and variance parameters of a normal distribution we can do this by constructing a target density function providing a sample of data and then running the sampler with sample generate a set of data from the posterior we want to estimate data rand normal 5 3 30 define the components of a basic model insupport 2 gt 0dist normal 1 2 density insupport sum logpdf dist data inf construct a densitymodel model densitymodel density set up our sampler with initial parameters spl metropolishastings 0 0 0 0 sample from the posterior chain sample model spl 100000 param names if all the interface functions have been extended properly you should get an output from display chain that looks something like this object of type chains with data of type 100000 3 1 array float64 3 iterations 1 100000thinning interval 1chains 1samples per chain 100000internals lpparameters 2 element array chaindataframe 1 summary statistics row parameters mean std naive se mcse ess r hat symbol float64 float64 float64 float64 any any 1 5 33157 0 854193 0 0027012 0 00893069 8344 75 1 00009 2 4 54992 0 632916 0 00200146 0 00534942 14260 8 1 00005 quantiles row parameters 2 5 25 0 50 0 75 0 97 5 symbol float64 float64 float64 float64 float64 1 3 6595 4 77754 5 33182 5 89509 6 99651 2 3 5097 4 09732 4 47805 4 93094 5 96821 it looks like we re extremely close to our true parameters of normal 5 3 though with a fairly high variance due to the low sample size conclusionwe ve seen how to implement the sampling interface for general projects turing s interface methods are ever evolving so please open an issue at abstractmcmc with feature requests or problems", "title": "Interface Guide"},{"location": "/docs/for-developers/variational_inference", "text": "overviewin this post we ll have a look at what s know as variational inference vi a family of approximate bayesian inference methods in particular we will focus on one of the more standard vi methods called automatic differentation variational inference advi ifhere we ll have a look at the theory behind vi but if you re interested in how to use advi in turing jl checkout this tutorial motivationin bayesian inference one usually specifies a model as follows given data x i i 1 n begin align text prior quad z amp sim p z text likelihood quad x i amp overset text i i d sim p x mid z quad text where quad i 1 dots n end align where overset text i i d sim denotes that the samples are identically independently distributed our goal in bayesian inference is then to find the posterior p z mid x i i 1 n prod i 1 n p z mid x i in general one cannot obtain a closed form expression for p z mid x i i 1 n but one might still be able to sample from p z mid x i i 1 n with guarantees of converging to the target posterior p z mid x i i 1 n as the number of samples go to infty e g mcmc as you are hopefully already aware turing jl provides a lot of different methods with asymptotic exactness guarantees that we can apply to such a problem unfortunately these unbiased samplers can be prohibitively expensive to run as the model p increases in complexity the convergence of these unbiased samplers can slow down dramatically still in the infinite limit these methods should converge to the true posterior but infinity is fairly large like at least more than 12 so this might take a while in such a case it might be desirable to sacrifice some of these asymptotic guarantees and instead approximate the posterior p z mid x i i 1 n using some other model which we ll denote q z there are multiple approaches to take in this case one of which is variational inference vi variational inference vi in vi we re looking to approximate p z mid x i i 1 n using some approximate or variational posterior q z to approximate something you need a notion of what close means in the context of probability densities a standard such measure of closeness is the kullback leibler kl divergence though this is far from the only one the kl divergence is defined between two densities q z and p z mid x i i 1 n as begin align mathrm d kl big q z p z mid x i i 1 n big amp int log bigg frac q z prod i 1 n p z mid x i bigg q z mathrm d z amp mathbb e z sim q z big log q z sum i 1 n log p z mid x i big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p z mid x i big end align it s worth noting that unfortunately the kl divergence is not a metric distance in the analysis sense due to its lack of symmetry on the other hand it turns out that minimizing the kl divergence that it s actually equivalent to maximizing the log likelihood also under reasonable restrictions on the densities at hand mathrm d kl big q z p z mid x i i 1 n big 0 quad iff quad q z p z mid x i i 1 n quad forall z therefore one could and we will attempt to approximate p z mid x i i 1 n using a density q z by minimizing the kl divergence between these two one can also show that mathrm d kl ge 0 which we ll need later finally notice that the kl divergence is only well defined when in fact q z is zero everywhere p z mid x i i 1 n is zero i e mathrm supp big q z big subseteq mathrm supp big p z mid x big otherwise there might be a point z 0 sim q z such that p z 0 mid x i i 1 n 0 resulting in log big frac q z 0 big which doesn t make sense one major problem as we can see in the definition of the kl divergence we need p z mid x i i 1 n for any z if we want to compute the kl divergence between this and q z we don t have that the entire reason we even do bayesian inference is that we don t know the posterior cleary this isn t going to work or is it computing kl divergence without knowing the posteriorfirst off recall that p z mid x i frac p x i z p x i so we can write begin align mathrm d kl big q z p z mid x i i 1 n big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p x i z log p x i big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p x i z big sum i 1 n mathbb e z sim q z big log p x i big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p x i z big sum i 1 n log p x i end align where in the last equality we used the fact that p x i is independent of z now you re probably thinking oh great now you ve introduced p x i which we also can t compute in general woah calm down human let s do some more algebra the above expression can be rearranged to mathrm d kl big q z p z mid x i i 1 n big underbrace sum i 1 n mathbb e z sim q z big log p x i z big mathbb e z sim q z big log q z big mathrm elbo q underbrace sum i 1 n mathbb e z sim q z big log p x i big text constant see the left hand side is constant and as we mentioned before mathrm d kl ge 0 what happens if we try to maximize the term we just gave the completely arbitrary name mathrm elbo well if mathrm elbo goes up while p x i stays constant then mathrm d kl has to go down that is the q z which minimizes the kl divergence is the same q z which maximizes mathrm elbo q underset q mathrm argmin mathrm d kl big q z p z mid x i i 1 n big underset q mathrm argmax mathrm elbo q where begin align mathrm elbo q amp bigg sum i 1 n mathbb e z sim q z big log p x i z big bigg mathbb e z sim q z big log q z big amp bigg sum i 1 n mathbb e z sim q z big log p x i z big bigg mathbb h big q z big end align and mathbb h big q z big denotes the differential entropy of q z assuming joint p x i z and the entropy mathbb h big q z big are both tractable we can use a monte carlo for the remaining expectation this leaves us with the following tractable expression underset q mathrm argmin mathrm d kl big q z p z mid x i i 1 n big approx underset q mathrm argmax widehat mathrm elbo q where widehat mathrm elbo q frac 1 m bigg sum k 1 m sum i 1 n log p x i z k bigg mathbb h big q z big quad text where quad z k sim q z quad forall k 1 dots m hence as long as we can sample from q z somewhat efficiently we can indeed minimize the kl divergence neat eh sidenote in the case where q z is tractable but mathbb h big q z big is not we can use an monte carlo estimate for this term too but this generally results in a higher variance estimate also i fooled you real good the elbo isn t an arbitrary name hah in fact it s an abbreviation for the expected lower bound elbo because it uhmm well it s the expected lower bound remember mathrm d kl ge 0 yup maximizing the elbofinding the optimal q over all possible densities of course isn t feasible instead we consider a family of parameterized densities mathscr d theta where theta denotes the space of possible parameters each density in this family q theta in mathscr d theta is parameterized by a unique theta in theta moreover we ll assume q theta z i e evaluating the probability density q at any point z is differentiable z sim q theta z i e the process of sampling from q theta z is differentiable 1 is fairly straight forward but 2 is a bit tricky what does it even mean for a sampling process to be differentiable this is quite an interesting problem in its own right and would require something like a 50 page paper to properly review the different approaches highly recommended read we re going to make use of a particular such approach which goes under a bunch of different names reparametrization trick path derivative etc this refers to making the assumption that all elements q theta in mathscr q theta can be considered as reparameterizations of some base density say bar q z that is if q theta in mathscr q theta then z sim q theta z quad iff quad z g theta tilde z quad text where quad bar z sim bar q z for some function g theta differentiable wrt theta so all q theta in mathscr q theta are using the same reparameterization function g but each q theta correspond to different choices of theta for f theta under this assumption we can differentiate the sampling process by taking the derivative of g theta wrt theta and thus we can differentiate the entire widehat mathrm elbo q theta wrt theta with the gradient available we can either try to solve for optimality either by setting the gradient equal to zero or maximize widehat mathrm elbo q theta stepwise by traversing mathscr q theta in the direction of steepest ascent for the sake of generality we re going to go with the stepwise approach with all this nailed down we eventually reach the section on automatic differentiation variational inference advi automatic differentiation variational inference advi so let s revisit the assumptions we ve made at this point the variational posterior q theta is in a parameterized family of densities denoted mathscr q theta with theta in theta mathscr q theta is a space of reparameterizable densities with bar q z as the base density the parameterization function g theta is differentiable wrt theta evaluation of the probability density q theta z is differentiable wrt theta mathbb h big q theta z big is tractable evaluation of the joint density p x z is tractable and differentiable wrt z the support of p z mid x is a subspace of the support of q z mathrm supp big p z mid x big subseteq mathrm supp big q z big all of these are not necessary to do vi but they are very convenient and results in a fairly flexible approach one distribution which has a density satisfying all of the above assumptions except 7 we ll get back to this in second for any tractable and differentiable p z mid x i i 1 n is the good ole gaussian normal distribution z sim mathcal n mu sigma quad iff quad z g mu l bar z mu l t tilde z quad text where quad bar z sim bar q z mathcal n 1 d i d times d where sigma l l t with l obtained from the cholesky decomposition abusing notation a bit we re going to write theta mu sigma mu 1 dots mu d l 11 dots l 1 d l 2 1 dots l 2 d dots l d 1 dots l d d with this assumption we finally have a tractable expression for widehat mathrm elbo q mu sigma well assuming 7 is holds since a gaussian has non zero probability on the entirety of mathbb r d we also require p z mid x i i 1 n to have non zero probability on all of mathbb r d though not necessary we ll often make a mean field assumption for the variational posterior q z i e assume independence between the latent variables in this case we ll write theta mu sigma 2 mu 1 dots mu d sigma 1 2 dots sigma d 2 examplesas a trivial example we could apply the approach described above to is the following generative model for p z mid x i i 1 n begin align m amp sim mathcal n 0 1 x i amp overset text i i d mathcal n m 1 quad i 1 dots n end align in this case z m and we have the posterior defined p m mid x i i 1 n p m prod i 1 n p x i mid m then the variational posterior would be q mu sigma mathcal n mu sigma 2 quad text where quad mu in mathbb r sigma 2 in mathbb r and since prior of m mathcal n 0 1 has non zero probability on the entirety of mathbb r same as q m i e assumption 7 above holds everything is fine and life is good but what about this generative model for p z mid x i i 1 n begin align s amp sim mathrm inversegamma 2 3 m amp sim mathcal n 0 s x i amp overset text i i d mathcal n m s quad i 1 dots n end align with posterior p s m mid x i i 1 n p s p m mid s prod i 1 n p x i mid s m and the mean field variational posterior q s m will be q mu 1 mu 2 sigma 1 2 sigma 2 2 s m p mathcal n mu 1 sigma 1 2 s p mathcal n mu 2 sigma 2 2 m where we ve denoted the evaluation of the probability density of a gaussian as p mathcal n mu sigma 2 x observe that mathrm inversegamma 2 3 has non zero probability only on mathbb r 0 infty which is clearly not all of mathbb r like q s m has i e mathrm supp big q s m big not subseteq mathrm supp big p z mid x i i 1 n big recall from the definition of the kl divergence that when this is the case the kl divergence isn t well defined this gets us to the automatic part of advi automatic how for a lot of the standard continuous densities p we can actually construct a probability density tilde p with non zero probability on all of mathbb r by transforming the constrained probability density p to tilde p in fact in these cases this is a one to one relationship as we ll see this helps solve the support issue we ve been going on and on about transforming densities using change of variablesif we want to compute the probability of x taking a value in some set a subseteq mathrm supp big p x big we have to integrate p x over a i e mathbb p p x in a int a p x mathrm d x this means that if we have a differentiable bijection f mathrm supp big q x big to mathbb r d with differentiable inverse f 1 mathbb r d to mathrm supp big p x big we can perform a change of variables mathbb p p x in a int f 1 a p big f 1 y big big det mathcal j f 1 y big mathrm d y where mathcal j f 1 x denotes the jacobian of f 1 evaluted at x observe that this defines a probability distribution mathbb p tilde p big y in f 1 a big int f 1 a tilde p y mathrm d y since f 1 big mathrm supp p x big mathbb r d which has probability 1 this probability distribution has density tilde p y with mathrm supp big tilde p y big mathbb r d defined tilde p y p big f 1 y big big det mathcal j f 1 y big or equivalently tilde p big f x big frac p x big det mathcal j f x big due to the fact that big det mathcal j f 1 y big big det mathcal j f x big 1 note it s also necessary that the log abs det jacobian term is non vanishing this can for example be accomplished by assuming f to also be elementwise monotonic back to viso why is this is useful well we re looking to generalize our approach using a normal distribution to cases where the supports don t match up how about defining q z by begin align eta amp sim mathcal n mu sigma z amp f 1 eta end align where f 1 mathbb r d to mathrm supp big p z mid x big is a differentiable bijection with differentiable inverse then z sim q mu sigma z implies z in mathrm supp big p z mid x big as we wanted the resulting variational density is q mu sigma z p mathcal n mu sigma big f z big big det mathcal j f z big note that the way we ve constructed q z here is basically a reverse of the approach we described above here we sample from a distribution with support on mathbb r and transform to mathrm supp big p z mid x big if we want to write the elbo explicitly in terms of eta rather than z the first term in the elbo becomes begin align mathbb e z sim q mu sigma z big log p x i z big amp mathbb e eta sim mathcal n mu sigma bigg log frac p big x i f 1 eta big big det mathcal j f 1 eta big bigg amp mathbb e eta sim mathcal n mu sigma big log p big x i f 1 eta big big mathbb e eta sim mathcal n mu sigma big big det mathcal j f 1 eta big big end align the entropy is invariant under change of variables thus mathbb h big q mu sigma z big is simply the entropy of the normal distribution which is known analytically hence the resulting empirical estimate of the elbo is begin align widehat mathrm elbo q mu sigma amp frac 1 m bigg sum k 1 m sum i 1 n big log p big x i f 1 eta k big log big det mathcal j f 1 eta k big big bigg mathbb h big p mathcal n mu sigma z big amp text where quad z k sim mathcal n mu sigma quad forall k 1 dots m end align and maximizing this wrt mu and sigma is what s referred to as automatic differentation variational inference advi now if you want to try it out check out the tutorial on how to use advi in turing jl", "title": "Variational Inference"},{"location": "/docs/library/advancedhmc/", "text": "index advancedhmc abstractintegrator advancedhmc abstractproposal advancedhmc abstracttrajectory advancedhmc abstracttrajectorysampler advancedhmc binarytree advancedhmc classicnouturn advancedhmc endpointts advancedhmc generalisednouturn advancedhmc hmcda advancedhmc jitteredleapfrog advancedhmc leapfrog advancedhmc multinomialts advancedhmc multinomialts advancedhmc multinomialts advancedhmc nuts advancedhmc nuts advancedhmc nuts advancedhmc slicets advancedhmc slicets advancedhmc slicets advancedhmc statictrajectory advancedhmc temperedleapfrog advancedhmc termination advancedhmc termination advancedhmc termination advancedhmc transition advancedhmc a advancedhmc build tree advancedhmc combine advancedhmc find good stepsize advancedhmc isterminated advancedhmc isterminated advancedhmc maxabs advancedhmc mh accept ratio advancedhmc nom step size advancedhmc pm next advancedhmc randcat advancedhmc simple pm next advancedhmc stat advancedhmc step size advancedhmc temper advancedhmc transitionfunctions advancedhmc a method a single hamiltonian integration step note this function is intended to be used in find good stepsize only advancedhmc build tree method recursivly build a tree for a given depth j advancedhmc combine method combine treeleft binarytree treeright binarytree merge a left tree treeleft and a right tree treeright under given hamiltonian h then draw a new candidate sample and update related statistics for the resulting tree advancedhmc find good stepsize method find a good initial leap frog step size via heuristic search advancedhmc isterminated method isterminated h hamiltonian t binarytree lt classicnouturn detect u turn for two phase points zleft and zright under given hamiltonian h using the original no u turn cirterion ref https arxiv org abs 1111 4246 https arxiv org abs 1701 02434 advancedhmc isterminated method isterminated h hamiltonian t binarytree lt generalisednouturn detect u turn for two phase points zleft and zright under given hamiltonian h using the generalised no u turn criterion ref https arxiv org abs 1701 02434 advancedhmc maxabs method maxabs a b return the value with the largest absolute value advancedhmc mh accept ratio method perform mh acceptance based on energy i e negative log probability advancedhmc nom step size method nom step size abstractintegrator get the nominal integration step size the current integration step size may differ from this for example if the step size is jittered nominal step size is usually used in adaptation advancedhmc pm next method progress meter update with all trajectory stats iteration number and metric shown advancedhmc randcat method randcat rng p abstractmatrix generating categorical random variables in a vectorized mode p is supposed to be a matrix of d n where each column is a probability vector examplep 0 5 0 3 0 4 0 6 0 1 0 1 u 0 3 0 4 c 0 5 0 3 0 9 0 9 1 0 1 0 then c lt u is 0 1 0 0 0 0 thus convert int vec sum c lt u dims 1 1 equals 1 2 advancedhmc simple pm next method simple progress meter update without any show values advancedhmc stat method returns the statistics for transition t advancedhmc step size function step size abstractintegrator get the current integration step size advancedhmc temper method temper lf temperedleapfrog r step namedtuple i is half lt tuple integer bool n steps int tempering step step is a named tuple with i being the current leapfrog iteration and is half indicating whether or not it s the first half momentum tempering step advancedhmc transition method transition abstracttrajectory i h hamiltonian z phasepoint make a mcmc transition from phase point z using the trajectory under hamiltonian h note this is a rng implicit fallback function for transition global rng h z statsbase sample method sample rng abstractrng h hamiltonian abstractproposal abstractvecormat t n samples int adaptor abstractadaptor noadaptation n adapts int min div n samples 10 1 000 drop warmup bool false verbose bool true progress bool false sample n samples samples using the proposal under hamiltonian h the randomness is controlled by rng if rng is not provided global rng will be used the initial point is given by the adaptor is set by adaptor for which the default is no adaptation it will perform n adapts steps of adaptation for which the default is the minimum of 1 000 and 10 of n samples drop warmup controls to drop the samples during adaptation phase or not verbose controls the verbosity progress controls whether to show the progress meter or nottypes advancedhmc abstractintegrator type abstract type abstractintegratorrepresents an integrator used to simulate the hamiltonian system implementationa abstractintegrator is expected to have the following implementations stat ref nom step size ref step size ref advancedhmc abstractproposal type abstract markov chain monte carlo proposal advancedhmc abstracttrajectory type hamiltonian dynamics numerical simulation trajectories advancedhmc abstracttrajectorysampler type defines how to sample a phase point from the simulated trajectory advancedhmc binarytree type a full binary tree trajectory with only necessary leaves and information stored advancedhmc classicnouturn type struct classicnouturn lt advancedhmc abstractterminationcriterionclassic no u turn criterion as described in eq 9 in 1 informally this will terminate the trajectory expansion if continuing the simulation either forwards or backwards in time will decrease the distance between the left most and right most positions references hoffman m d amp gelman a 2014 the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo journal of machine learning research 15 1 1593 1623 arxiv advancedhmc endpointts type struct endpointts lt advancedhmc abstracttrajectorysamplersamples the end point of the trajectory advancedhmc generalisednouturn type struct generalisednouturn t lt abstractarray var s58 1 where var s58 lt real lt advancedhmc abstractterminationcriteriongeneralised no u turn criterion as described in section a 4 2 in 1 fields rho abstractarray var s58 1 where var s58 lt real integral or sum of momenta along the integration path references betancourt m 2017 a conceptual introduction to hamiltonian monte carlo arxiv preprint arxiv 1701 02434 advancedhmc hmcda type struct hmcda s lt advancedhmc abstracttrajectorysampler i lt advancedhmc abstractintegrator lt advancedhmc dynamictrajectory i lt advancedhmc abstractintegrator standard hmc implementation with fixed total trajectory length fields integrator advancedhmc abstractintegrator integrator used to simulate trajectory abstractfloat total length of the trajectory i e take floor integrator step number of leapfrog steps references neal r m 2011 mcmc using hamiltonian dynamics handbook of markov chain monte carlo 2 11 2 arxiv advancedhmc jitteredleapfrog type struct jitteredleapfrog ft lt abstractfloat t lt union abstractarray ft lt abstractfloat 1 ft lt abstractfloat lt advancedhmc abstractleapfrog t lt union abstractarray ft lt abstractfloat 1 ft lt abstractfloat leapfrog integrator with randomly jittered step size for every trajectory fields 0 union abstractarray ft 1 ft where ft lt abstractfloat nominal non jittered step size jitter abstractfloat the proportion of the nominal step size 0 that may be added or subtracted union abstractarray ft 1 ft where ft lt abstractfloat current jittered step size descriptionthis is the same as leapfrog ref but with a jittered step size this means that at the beginning of each trajectory we sample a step size by adding or subtracting from the nominal base step size 0 some random proportion of 0 with the proportion specified by jitter i e 0 jitter 0 rand p jittering might help alleviate issues related to poor interactions with a fixed step size in regions with high curvature the current choice of step size might mean over shoot leading to almost all steps being rejected randomly sampling the step size at the beginning of the trajectories can therefore increase the probability of escaping such high curvature regions exact periodicity of the simulated trajectories might occur i e you might be so unlucky as to simulate the trajectory forwards in time l and ending up at the same point which results in non ergodicity see section 3 2 in 1 if momentum is refreshed before each trajectory then this should not happen exactly but it can still be an issue in practice randomly choosing the step size might help alleviate such problems references neal r m 2011 mcmc using hamiltonian dynamics handbook of markov chain monte carlo 2 11 2 arxiv advancedhmc leapfrog type struct leapfrog t lt union abstractarray var s58 1 var s58 where var s58 lt abstractfloat lt advancedhmc abstractleapfrog t lt union abstractarray var s58 1 var s58 where var s58 lt abstractfloat leapfrog integrator with fixed step size fields union abstractarray var s58 1 var s58 where var s58 lt abstractfloat step size advancedhmc multinomialts type struct multinomialts f lt abstractfloat lt advancedhmc abstracttrajectorysamplermultinomial trajectory sampler carried during the building of the tree it contains the weight of the tree defined as the total probabilities of the leaves fields zcand advancedhmc phasepoint sampled candidate phasepoint w abstractfloat total energy for the given tree i e the sum of energies of all leaves advancedhmc multinomialts method multinomialts s multinomialts h0 abstractfloat zcand phasepoint multinomial sampler for a trajectory consisting only a leaf node tree weight is the unnormalised energy of the leaf advancedhmc multinomialts method multinomialts rng abstractrng z0 phasepoint multinomial sampler for the starting single leaf tree log weights for leaf nodes are their unnormalised hamiltonian energies ref https github com stan dev stan blob develop src stan mcmc hmc nuts base nuts hpp l226 advancedhmc nuts type dynamic trajectory hmc using the no u turn termination criteria algorithm advancedhmc nuts method nuts args nuts multinomialts generalisednouturn args create an instance for the no u turn sampling algorithm with multinomial sampling and original no u turn criterion below is the doc for nuts s c nuts s c integrator i max depth int 10 max f 1000 0 where i lt abstractintegrator f lt abstractfloat s lt abstracttrajectorysampler c lt abstractterminationcriterion create an instance for the no u turn sampling algorithm advancedhmc nuts method nuts s c integrator i max depth int 10 max f 1000 0 where i lt abstractintegrator f lt abstractfloat s lt abstracttrajectorysampler c lt abstractterminationcriterion create an instance for the no u turn sampling algorithm advancedhmc slicets type struct slicets f lt abstractfloat lt advancedhmc abstracttrajectorysamplertrajectory slice sampler carried during the building of the tree it contains the slice variable and the number of acceptable condidates in the tree fields zcand advancedhmc phasepoint sampled candidate phasepoint u abstractfloat slice variable in log space n int64 number of acceptable candidates i e those with probability larger than slice variable u advancedhmc slicets method slicets rng abstractrng z0 phasepoint slice sampler for the starting single leaf tree slice variable is initialized advancedhmc slicets method slicets s slicets h0 abstractfloat zcand phasepoint create a slice sampler for a single leaf tree the slice variable is copied from the passed in sampler s and the number of acceptable candicates is computed by comparing the slice variable against the current energy advancedhmc statictrajectory type struct statictrajectory s lt advancedhmc abstracttrajectorysampler i lt advancedhmc abstractintegrator lt advancedhmc abstracttrajectory i lt advancedhmc abstractintegrator static hmc with a fixed number of leapfrog steps fields integrator advancedhmc abstractintegrator integrator used to simulate trajectory n steps int64 number of steps to simulate i e length of trajectory will be n steps 1 references neal r m 2011 mcmc using hamiltonian dynamics handbook of markov chain monte carlo 2 11 2 arxiv advancedhmc temperedleapfrog type struct temperedleapfrog ft lt abstractfloat t lt union abstractarray ft lt abstractfloat 1 ft lt abstractfloat lt advancedhmc abstractleapfrog t lt union abstractarray ft lt abstractfloat 1 ft lt abstractfloat tempered leapfrog integrator with fixed step size and temperature fields union abstractarray ft 1 ft where ft lt abstractfloat step size abstractfloat temperature parameter descriptiontempering can potentially allow greater exploration of the posterior e g in a multi modal posterior jumps between the modes can be more likely to occur advancedhmc termination type terminationtermination reasons dynamic due to stoping criteria numerical due to large energy deviation from starting possibly numerical errors advancedhmc termination method termination s multinomialts nt nuts h0 f h f where f lt abstractfloat check termination of a hamiltonian trajectory advancedhmc termination method termination s slicets nt nuts h0 f h f where f lt abstractfloat check termination of a hamiltonian trajectory advancedhmc transition type struct transition p lt advancedhmc phasepoint nt lt namedtuple a transition that contains the phase point and other statistics of the transition fields z advancedhmc phasepoint phase point for the transition stat namedtuple statistics related to the transition e g energy", "title": "AdvancedHMC"},{"location": "/docs/library/", "text": "index turing binomiallogit turing flat turing flatpos turing orderedlogistic turing inference gibbs turing inference hmc turing inference hmcda turing inference is turing inference mh turing inference nuts turing inference pg turing inference smc libtask tarray libtask tzerosmodelling dynamicppl model macro model expr warn true macro to specify a probabilistic model if warn is true a warning is displayed if internal variable names are used in the model definition examplesmodel definition model function model x y 42 endto generate a model call model xvalue or model xvalue yvalue samplers dynamicppl sampler type sampler t generic interface for implementing inference algorithms an implementation of an algorithm should include the following a type specifying the algorithm and its parameters derived from inferencealgorithm a method of sample function that produces results of inference which is where actual inference happens dynamicppl translates models to chunks that call the modelling functions at specified points the dispatch is based on the value of a sampler variable to include a new inference algorithm implements the requirements mentioned above in a separate file then include that file at the end of this one turing inference gibbs type gibbs algs compositional mcmc interface gibbs sampling combines one or more sampling algorithms each of which samples from a different set of variables in a model example model gibbs example x begin v1 normal 0 1 v2 categorical 5 enduse pg for a v2 variable and use hmc for the v1 variable note that v2 is discrete so the pg sampler is more appropriatethan is hmc alg gibbs hmc 0 2 3 v1 pg 20 v2 tips hmc and nuts are fast samplers and can throw off particle basedmethods like particle gibbs you can increase the effectiveness of particle sampling by including more particles in the particle sampler source turing inference hmc type hmc float64 n leapfrog int hamiltonian monte carlo sampler with static trajectory arguments float64 the leapfrog step size to use n leapfrog int the number of leapfrop steps to use usage hmc 0 05 10 tips if you are receiving gradient errors when using hmc try reducing the leapfrog step size e g original step sizesample gdemo 1 5 2 hmc 0 1 10 1000 reduced step sizesample gdemo 1 5 2 hmc 0 01 10 1000 source turing inference hmcda type hmcda n adapts int float64 float64 float64 0 0 hamiltonian monte carlo sampler with dual averaging algorithm usage hmcda 200 0 65 0 3 arguments n adapts int numbers of samples to use for adaptation float64 target acceptance rate 65 is often recommended float64 target leapfrop length float64 0 0 inital step size 0 means automatically search by turing for more information please view the following paper arxiv link hoffman matthew d and andrew gelman the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo journal of machine learning research 15 no 1 2014 1593 1623 source turing inference is type is importance sampling algorithm note that this method is particle based and arrays of variables must be stored in a tarray object usage is example define a simple normal model with unknown mean and variance model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s x 1 normal m sqrt s x 2 normal m sqrt s return s mendsample gdemo 1 5 2 is 1000 source turing inference mh type mh space construct a metropolis hastings algorithm the arguments space can be blank i e mh in which case mh defaults to using the prior for each parameter as the proposal distribution a set of one or more symbols to sample with mh in conjunction with gibbs i e gibbs mh m pg 10 s an iterable of pairs or tuples mapping a symbol to a advancedmh proposal distribution or function that generates returns a conditional proposal distribution a covariance matrix to use as for mean zero multivariate normal proposals examplesthe default mh will use propose samples from the prior distribution using advancedmh staticproposal model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endchain sample gdemo 1 5 2 0 mh 1 000 mean chain alternatively you can specify particular parameters to sample if you want to combine sampling from multiple samplers model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end samples s with mh and m with pgchain sample gdemo 1 5 2 0 gibbs mh s pg 10 m 1 000 mean chain using custom distributions defaults to using static mh julia model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s enduse a static proposal for s and random walk with proposalstandard deviation of 0 25 for m chain sample gdemo 1 5 2 0 mh s gt inversegamma 2 3 m gt normal 0 1 1 000 mean chain specifying explicit proposals using the advancedmh interface julia model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end use a static proposal for s and random walk with proposal standard deviation of 0 25 for m chain sample gdemo 1 5 2 0 mh s gt advancedmh staticproposal inversegamma 2 3 m gt advancedmh randomwalkproposal normal 0 0 25 1 000 mean chain using a custom function to specify a conditional distribution model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end use a static proposal for s and and a conditional proposal for m where the proposal is centered around the current sample chain sample gdemo 1 5 2 0 mh s gt inversegamma 2 3 m gt x gt normal x 1 1 000 mean chain providing a covariance matrix will cause mh to perform random walksampling in the transformed space with proposals drawn from a multivariatenormal distribution the provided matrix must be positive semi definite and square usage julia model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endproviding a custom variance covariance matrixchain sample gdemo 1 5 2 0 mh 0 25 0 05 0 05 0 50 1 000 mean chain source turing inference nuts type nuts n adapts int float64 max depth int 5 max float64 1000 0 float64 0 0 no u turn sampler nuts sampler usage nuts use default nuts configuration nuts 1000 0 65 use 1000 adaption steps and target accept ratio 0 65 arguments n adapts int the number of samples to use with adaptation float64 target acceptance rate for dual averaging max depth int maximum doubling tree depth max float64 maximum divergence during doubling tree float64 inital step size 0 means automatically searching using a heuristic procedure source turing inference pg type struct pg space r lt turing inference particleinferenceparticle gibbs sampler note that this method is particle based and arrays of variables must be stored in a tarray object fields nparticles int64 number of particles resampler any resampling algorithm source turing inference smc type struct smc space r lt turing inference particleinferencesequential monte carlo sampler fields resampler anysourcedistributions turing flat type flat lt continuousunivariatedistributiona distribution with support and density of one everywhere source turing flatpos type flatpos l real a distribution with a lower bound of l and a density of one at every x above l source turing binomiallogit type binomiallogit n lt real i lt integer a univariate binomial logit distribution source warning missing docstring missing docstring for vecbinomiallogit check documenter s build log for details turing orderedlogistic type orderedlogistic any cutpoints lt abstractvector an ordered logistic distribution sourcedata structures libtask tarray type tarray t dims implementation of data structures that automatically perform copy on write after task copying if current task is an existing key in s then return s current task otherwise returns current task s last task usage tarray dim example ta tarray 4 initfor i in 1 4 ta i i end assignarray ta convert to 4 element array int64 1 1 2 3 4 utilities libtask tzeros function tzeros dims construct a distributed array of zeros trailing arguments are the same as those accepted by tarray tzeros dim example tz tzeros 4 constructarray tz convert to 4 element array int64 1 0 0 0 0", "title": "API"},{"location": "/docs/library/bijectors/", "text": "index bijectors adbijector bijectors abstractbijector bijectors bijector bijectors composed bijectors corrbijector bijectors inverse bijectors permute bijectors stacked bijectors link chol lkj bijectors bijector bijectors composel bijectors composer bijectors compute r bijectors find alpha bijectors forward bijectors forward bijectors isclosedform bijectors logabsdetjac bijectors logabsdetjac bijectors logabsdetjacinv bijectors logabsdetjacinv bijectors logpdf with jac bijectors transformedfunctions bijectors link chol lkj method function link chol lkj w link function for cholesky factor an alternative and maybe more efficient implementation was considered for i 2 k j i 1 k z i j w i j w i 1 j z i 1 j sqrt 1 z i 1 j 2 endbut this implementation will not work when w i 1 j 0 though it is a zero measure set unit matrix initialization will not work for equivelence following explanations is given by torfjelde for i j in the loop below we definez w 1 1 z and soz w 1 1 z w 1 z 1 1 z w 1 z w 1 1 z w w 1 z z w w w z 1 z which is the above implementation bijectors bijector method bijector d distribution returns the constrained to unconstrained bijector for distribution d bijectors composel method composel ts bijector composed lt tuple constructs composed such that ts are applied left to right bijectors composer method composer ts bijector composed lt tuple constructs composed such that ts are applied right to left bijectors compute r method compute r y minus z0 abstractvector lt real plus hat compute the unique solution r to the equation y minus z0 2 r left 1 frac plus hat r right subject to r 0 and r since gt 0 and plus hat gt 0 the solution is unique and given by r sqrt plus hat 2 4 plus hat 2 where y minus z0 2 for details see appendix a 2 of the reference referencesd rezende s mohamed 2015 variational inference with normalizing flows arxiv 1505 05770 bijectors find alpha method find alpha y abstractvector lt real wt y wt u hat b compute an approximate real valued solution to the equation wt y wt u hat tanh b the uniqueness of the solution is guaranteed since wt u hat 1 for details see appendix a 1 of the reference referencesd rezende s mohamed 2015 variational inference with normalizing flows arxiv 1505 05770 bijectors forward method forward b bijector x computes both transform and logabsdetjac in one forward pass and returns a named tuple rv b x logabsdetjac logabsdetjac b x this defaults to the call above but often one can re use computation in the computation of the forward pass and the computation of the logabsdetjac forward allows the user to take advantange of such efficiencies if they exist bijectors forward method forward d distribution forward d distribution num samples int returns a namedtuple with fields x y logabsdetjac and logpdf in the case where d isa transformeddistribution this means x rand d dist y d transform x logabsdetjac is the logabsdetjac of the forward transform logpdf is the logpdf of y not xin the case where d isa distribution this means x rand d y x logabsdetjac 0 0 logpdf is logpdf of x bijectors isclosedform method isclosedform b bijector boolisclosedform b inverse lt bijector boolreturns true or false depending on whether or not evaluation of b has a closed form implementation most bijectors have closed form evaluations but there are cases where this is not the case for example the inverse evaluation of planarlayer requires an iterative procedure to evaluate and thus is not differentiable bijectors logabsdetjac method computes the absolute determinant of the jacobian of the inverse transformation bijectors logabsdetjac method logabsdetjac b bijector x logabsdetjac ib inverse lt bijector y computes the log abs det j b x where j is the jacobian of the transform similarily for the inverse transform default implementation for inverse lt bijector is implemented as logabsdetjac of original bijector bijectors logabsdetjacinv method logabsdetjacinv b bijector y just an alias for logabsdetjac inv b y bijectors logabsdetjacinv method logabsdetjacinv td univariatetransformed y real logabsdetjacinv td multivariatetransformed y abstractvector lt real computes the logabsdetjac of the inverse transformation since rand td returns the transformed random variable bijectors logpdf with jac method logpdf with jac td univariatetransformed y real logpdf with jac td mvtransformed y abstractvector lt real logpdf with jac td matrixtransformed y abstractmatrix lt real makes use of the forward method to potentially re use computation and returns a tuple logpdf logabsdetjac bijectors transformed method transformed d distribution transformed d distribution b bijector couples distribution d with the bijector b by returning a transformeddistribution if no bijector is provided i e transformed d is called then transformed d bijector d is returned types bijectors adbijector type abstract type for a bijector n making use of auto differentation ad to implement jacobian and by impliciation logabsdetjac bijectors abstractbijector type abstract type for a bijector bijectors bijector type abstract type of bijectors with fixed dimensionality bijectors composed type composed ts a b1 bijector n b2 bijector n composed lt tuple composel ts bijector n composed lt tuple composer ts bijector n composed lt tuple where a refers to either tuple vararg lt bijector n a tuple of bijectors of dimensionality n abstractarray lt bijector n an array of bijectors of dimensionality na bijector representing composition of bijectors composel and composer results in a composed for which application occurs from left to right and right to left respectively note that all the alternative ways of constructing a composed returns a tuple of bijectors this ensures type stability of implementations of all relating methdos e g inv if you want to use an array as the container instead you can docomposed b1 b2 in general this is not advised since you lose type stability but there might be cases where this is desired e g if you have a insanely large number of bijectors to compose examplessimple examplelet s consider a simple example of exp julia gt using bijectors expjulia gt b exp exp 0 julia gt b bcomposed tuple exp 0 exp 0 0 exp 0 exp 0 julia gt b b 1 0 exp exp 1 0 evaluationtruejulia gt inv b b exp exp 1 0 1 0 inversiontruejulia gt logabsdetjac b b 1 0 determinant of jacobian3 718281828459045notesorderit s important to note that does what is expected mathematically which means that the bijectors are applied to the input right to left e g first applying b2 and then b1 b1 b2 x b1 b2 x gt truebut in the composed struct itself we store the bijectors left to right so thatcb1 b1 b2 gt composed ts b2 b1 cb2 composel b2 b1 gt composed ts b2 b1 cb1 x cb2 x b1 b2 x gt truestructure will result in flatten the composition structure while composel and composer preserve the compositional structure this is most easily seen by an example julia gt b exp exp 0 julia gt cb1 b b cb2 b b julia gt cb1 cb2 ts lt different exp 0 exp 0 exp 0 exp 0 julia gt cb1 cb2 ts isa ntuple 4 exp 0 truejulia gt bijectors composer cb1 cb2 ts composed tuple exp 0 exp 0 0 exp 0 exp 0 composed tuple exp 0 exp 0 0 exp 0 exp 0 julia gt bijectors composer cb1 cb2 ts isa tuple composed composed true bijectors corrbijector type corrbijector lt bijector 2 a bijector implementation of stan s parametrization method for correlation matrix https mc stan org docs 2 23 reference manual correlation matrix transform section htmlbasically a unconstrained strictly upper triangular matrix y is transformed to a correlation matrix by following readable but not that efficient form k size y 1 z tanh y for j 1 k i 1 k if i gt j w i j 0 elseif 1 i j w i j 1 elseif 1 lt i j w i j prod sqrt 1 z 1 i 1 j 2 elseif 1 i lt j w i j z i j elseif 1 lt i lt j w i j z i j prod sqrt 1 z 1 i 1 j 2 endendit is easy to see that every column is a unit vector for example w3 w3 w 1 3 2 w 2 3 2 w 3 3 2 z 1 3 2 z 2 3 sqrt 1 z 1 3 2 2 sqrt 1 z 1 3 2 sqrt 1 z 2 3 2 2 z 1 3 2 z 2 3 2 1 z 1 3 2 1 z 1 3 2 1 z 2 3 2 z 1 3 2 z 2 3 2 z 2 3 2 z 1 3 2 1 z 1 3 2 z 2 3 2 z 1 3 2 z 2 3 2 1and diagonal elements are positive so w is a cholesky factor for a positive matrix x w wconsider block matrix representation for xx w1 w2 wn w1 w2 wn w1 w1 w1 w2 w1 wn w2 w1 w2 w2 w2 wn the diagonal elements are given by wk wk 1 thus x is a correlation matrix every step is invertible so this is a bijection bijector note the implementation doesn t follow their manageable expression directly because their equation seems wrong 7 30 2020 insteadly it follows definition above the manageable expression directly which is also described in above doc bijectors inverse type inv b bijector inverse b bijector a bijector representing the inverse transform of b bijectors permute type permute a lt bijector 1 a bijector implementation of a permutation the permutation is performed using a matrix of type a there are a couple of different ways to construct permute permute 0 1 1 0 will map 1 2 gt 2 1 permute 2 1 will map 1 2 gt 2 1 permute 2 2 gt 1 1 gt 2 will map 1 2 gt 2 1 permute 2 1 2 gt 2 1 will map 1 2 gt 2 1 if this is not clear the examples might be of help examplesa simple example is permuting a vector of size 3 julia gt b1 permute 0 1 0 1 0 0 0 0 1 permute array int64 2 0 1 0 1 0 0 0 0 1 julia gt b2 permute 2 1 3 specify all elements at oncepermute sparsearrays sparsematrixcsc float64 int64 2 1 1 0 1 2 1 0 3 3 1 0 julia gt b3 permute 3 2 gt 1 1 gt 2 element wisepermute sparsearrays sparsematrixcsc float64 int64 2 1 1 0 1 2 1 0 3 3 1 0 julia gt b4 permute 3 1 2 gt 2 1 block wisepermute sparsearrays sparsematrixcsc float64 int64 2 1 1 0 1 2 1 0 3 3 1 0 julia gt b1 a b2 a b3 a b4 atruejulia gt b1 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt b2 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt b3 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt b4 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt inv b1 permute linearalgebra transpose int64 array int64 2 0 1 0 1 0 0 0 0 1 julia gt inv b1 b1 1 2 3 3 element array float64 1 1 0 2 0 3 0 bijectors stacked type stacked bs stacked bs ranges stack bs bijector 0 where 0 means 0 dim bijector a bijector which stacks bijectors together which can then be applied to a vector where bs i bijector is applied to x ranges i unitrange int arguments bs can be either a tuple or an abstractarray of 0 and or 1 dimensional bijectors if bs is a tuple implementations are type stable using generated functions if bs is an abstractarray implementations are not type stable and use iterative methods ranges needs to be an iterable consisting of unitrange int length bs length ranges needs to be true examplesb1 logit 0 0 1 0 b2 identity 0 b stack b1 b2 b 0 0 1 0 b1 0 0 1 0 gt true", "title": "Bijectors"},{"location": "/docs/tutorials/index", "text": "tutorialsthis section contains tutorials on how to implement common models in turing if you prefer to have an interactive jupyter notebook please fork or download the turingtutorials repository a list of all the tutorials available can be found to the left the introduction tutorial contains an introduction to coin flipping with turing and a brief overview of probabalistic programming tutorials are under continuous development but there are some older version available at the turingtutorials within the old notebooks section some of these were built using prior versions of turing and may not function correctly but they can assist in the syntax used for common models if there is a tutorial you would like to request please open an issue on the turingtutorials repository", "title": "Tutorials"},{"location": "/docs/using-turing/advanced", "text": "advanced usagehow to define a customized distributionturing jl supports the use of distributions from the distributions jl package by extension it also supports the use of customized distributions by defining them as subtypes of distribution type of the distributions jl package as well as corresponding functions below shows a workflow of how to define a customized distribution using our own implementation of a simple uniform distribution as a simple example 1 define the distribution typefirst define a type of the distribution as a subtype of a corresponding distribution type in the distributions jl package struct customuniform lt continuousunivariatedistributionend2 implement sampling and evaluation of the log pdfsecond define rand and logpdf which will be used to run the model distributions rand rng abstractrng d customuniform rand rng sample in 0 1 distributions logpdf d customuniform x real zero x p x 1 logp x 03 define helper functionsin most cases it may be required to define some helper functions 3 1 domain transformationcertain samplers such as hmc require the domain of the priors to be unbounded therefore to use our customuniform as a prior in a model we also need to define how to transform samples from 0 1 to to do this we simply need to define the corresponding bijector from bijectors jl which is what turing jl uses internally to deal with constrained distributions to transform from 0 1 to we can use the logit bijector bijectors bijector d customuniform logit 0 1 you d do the exact same thing for continuousmultivariatedistribution and continuousmatrixdistribution for example wishart defines a distribution over positive definite matrices and so bijector returns a pdbijector when called with a wishart distribution as an argument for discrete distributions there is no need to define a bijector the identity bijector is used by default alternatively for univariatedistribution we can define the minimum and maximum of the distributiondistributions minimum d customuniform 0 distributions maximum d customuniform 1 and bijectors jl will return a default bijector called truncatedbijector which makes use of minimum and maximum derive the correct transformation internally turing basically does the following when it needs to convert a constrained distribution to an unconstrained distribution e g when sampling using hmc b bijector dist transformed dist transformed dist b results in distribution with transformed support correction for logpdfand then we can call rand and logpdf as usual where rand transformed dist returns a sample in the unconstrained space and logpdf transformed dist y returns the log density of the original distribution but with y living in the unconstrained space to read more about bijectors jl check out the project readme 3 2 vectorization supportthe vectorization syntax follows rv distribution which requires rand and logpdf to be called on multiple data points at once an appropriate implementation for flat is shown below distributions logpdf d flat x abstractvector lt real zero x update the accumulated log probability in the model definitionturing accumulates log probabilities internally in an internal data structure that is accessible through the internal variable varinfo inside of the model definition see below for more details about model internals however since users should not have to deal with internal data structures a macro turing addlogprob is provided that increases the accumulated log probability for instance this allows you to include arbitrary terms in the likelihoodusing turingmyloglikelihood x loglikelihood normal 1 x model function demo x normal turing addlogprob myloglikelihood x endand to reject samples using turingusing linearalgebra model function demo x m mvnormal length x if dot m x lt 0 turing addlogprob inf exit the model evaluation early return end x mvnormal m 1 0 returnendnote that addlogprob always increases the accumulated log probability regardless of the provided sampling context for instance if you do not want to apply turing addlogprob when evaluating the prior of your model but only when computing the log likelihood and the log joint probability then you should check the type of the internal variable context such asif isa context turing priorcontext turing addlogprob myloglikelihood x endmodel internalsthe model macro accepts a function definition and rewrites it such that call of the function generates a model struct for use by the sampler models can be constructed by hand without the use of a macro taking the gdemo model as an example the macro based definitionusing turing model function gdemo x set priors s inversegamma 2 3 m normal 0 sqrt s observe each value of x x normal m sqrt s endmodel gdemo 1 5 2 0 is equivalent to the macro free versionusing turing create the model function function modelf rng model varinfo sampler context x assume s has an inversegamma distribution s turing dynamicppl tilde assume rng context sampler inversegamma 2 3 turing varname s varinfo assume m has a normal distribution m turing dynamicppl tilde assume rng context sampler normal 0 sqrt s turing varname m varinfo observe each value of x i according to a normal distribution turing dynamicppl dot tilde observe context sampler normal m sqrt s x varinfo end instantiate a model object with our data variables model turing model modelf x 1 5 2 0 task copyingturing copies julia tasks to deliver efficient inference algorithms but it also provides alternative slower implementation as a fallback task copying is enabled by default task copying requires us to use the ctask facility which is provided by libtask to create tasks", "title": "Advanced Usage"},{"location": "/docs/using-turing/autodiff", "text": "automatic differentiationswitching ad modesturing supports four packages of automatic differentiation ad in the back end during sampling the default ad backend is forwarddiff for forward mode ad three reverse mode ad backends are also supported namely tracker zygote and reversediff zygote and reversediff are supported optionally if explicitly loaded by the user with using zygote or using reversediff next to using turing to switch between the different ad backends one can call function turing setadbackend backend sym where backend sym can be forwarddiff forwarddiff tracker tracker zygote zygote or reversediff reversediff jl when using reversediff to compile the tape only once and cache it for later use the user needs to load memoization jl first with using memoization then call turing setrdcache true however note that the use of caching in certain types of models can lead to incorrect results and or errors models for which the compiled tape can be safely cached are models with fixed size loops and no run time if statements compile time if statements are fine to empty the cache you can call turing emptyrdcache compositional sampling with differing ad modesturing supports intermixed automatic differentiation methods for different variable spaces the snippet below shows using forwarddiff to sample the mean m parameter and using the tracker based trackerad autodiff for the variance s parameter using turing define a simple normal model with unknown mean and variance model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end sample using gibbs and varying autodiff backends c sample gdemo 1 5 2 gibbs hmc turing forwarddiffad 1 0 1 5 m hmc turing trackerad 0 1 5 s 1000 generally trackerad is faster when sampling from variables of high dimensionality greater than 20 and forwarddiffad is more efficient for lower dimension variables this functionality allows those who are performance sensistive to fine tune their automatic differentiation for their specific models if the differentation method is not specified in this way turing will default to using whatever the global ad backend is currently this defaults to forwarddiff", "title": "Automatic Differentiation"},{"location": "/docs/using-turing/dynamichmc", "text": "using dynamichmcturing supports the use of dynamichmc as a sampler through the dynamicnuts function dynamicnuts is not appropriate for use in compositional inference if you intend to use gibbs sampling you must use turing s native nuts function to use the dynamicnuts function you must import the dynamichmc package as well as turing turing does not formally require dynamichmc but will include additional functionality if both packages are present here is a brief example of how to apply dynamicnuts import turing and dynamichmc using logdensityproblems dynamichmc turing model definition model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end pull 2 000 samples using dynamicnuts chn sample gdemo 1 5 2 0 dynamicnuts 2000", "title": "Using DynamicHMC"},{"location": "/docs/using-turing/get-started", "text": "getting startedinstallationto use turing you need to install julia first and then install turing install juliayou will need to install julia 1 3 or greater which you can get from the official julia website install turing jlturing is an officially registered julia package so you can install a stable version of turing by running the following in the julia repl julia gt add turingyou can check if all tests pass by runningjulia gt test turingexamplehere s a simple example showing the package in action using turingusing statsplots define a simple normal model with unknown mean and variance model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end run sampler collect resultschn sample gdemo 1 5 2 hmc 0 1 5 1000 summarise resultsdescribe chn plot and save resultsp plot chn savefig gdemo plot png", "title": "Getting Started"},{"location": "/docs/using-turing/guide", "text": "guidebasicsintroductiona probabilistic program is julia code wrapped in a model macro it can use arbitrary julia code but to ensure correctness of inference it should not have external effects or modify global state stack allocated variables are safe but mutable heap allocated objects may lead to subtle bugs when using task copying to help avoid those we provide a turing safe datatype tarray that can be used to create mutable arrays in turing programs to specify distributions of random variables turing programs should use the notation x distr where x is a symbol and distr is a distribution if x is undefined in the model function inside the probabilistic program this puts a random variable named x distributed according to distr in the current scope distr can be a value of any type that implements rand distr which samples a value from the distribution distr if x is defined this is used for conditioning in a style similar to anglican another ppl in this case x is an observed value assumed to have been drawn from the distribution distr the likelihood is computed using logpdf distr y the observe statements should be arranged so that every possible run traverses all of them in exactly the same order this is equivalent to demanding that they are not placed inside stochastic control flow available inference methods include importance sampling is sequential monte carlo smc particle gibbs pg hamiltonian monte carlo hmc hamiltonian monte carlo with dual averaging hmcda and the no u turn sampler nuts simple gaussian demobelow is a simple gaussian demo illustrate the basic usage of turing jl import packages using turingusing statsplots define a simple normal model with unknown mean and variance model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endnote as a sanity check the expectation of s is 49 24 2 04166666 and the expectation of m is 7 6 1 16666666 we can perform inference by using the sample function the first argument of which is our probabalistic program and the second of which is a sampler more information on each sampler is located in the api run sampler collect results c1 sample gdemo 1 5 2 smc 1000 c2 sample gdemo 1 5 2 pg 10 1000 c3 sample gdemo 1 5 2 hmc 0 1 5 1000 c4 sample gdemo 1 5 2 gibbs pg 10 m hmc 0 1 5 s 1000 c5 sample gdemo 1 5 2 hmcda 0 15 0 65 1000 c6 sample gdemo 1 5 2 nuts 0 65 1000 the mcmcchains module which is re exported by turing provides plotting tools for the chain objects returned by a sample function see the mcmcchains repository for more information on the suite of tools available for diagnosing mcmc chains summarise resultsdescribe c3 plot resultsplot c3 savefig gdemo plot png the arguments for each sampler are smc number of particles pg number of particles number of iterations hmc leapfrog step size leapfrog step numbers gibbs component sampler 1 component sampler 2 hmcda total leapfrog length target accept ratio nuts number of adaptation steps optional target accept ratio for detailed information on the samplers please review turing jl s api documentation modelling syntax explainedusing this syntax a probabilistic model is defined in turing the model function generated by turing can then be used to condition the model onto data subsequently the sample function can be used to generate samples from the posterior distribution in the following example the defined model is conditioned to the date arg1 1 arg2 2 by passing 1 2 to the model function model function model name arg 1 arg 2 endthe conditioned model can then be passed onto the sample function to run posterior inference model func model name 1 2 chn sample model func hmc perform inference by sampling using hmc the returned chain contains samples of the variables in the model var 1 mean chn var 1 taking the mean of a variable named var 1 the key var 1 can be a symbol or a string for example to fetch x 1 one can use chn symbol x 1 or chn x 1 if you want to retrieve all parameters associated with a specific symbol you can use group as an example if you have the parameters x 1 x 2 and x 3 calling group chn x or group chn x will return a new chain with only x 1 x 2 and x 3 turing does not have a declarative form more generally the order in which you place the lines of a model macro matters for example the following example works define a simple normal model with unknown mean and variance model function model function y s poisson 1 y normal s 1 return yendsample model function 10 smc 100 but if we switch the s poisson 1 and y normal s 1 lines the model will no longer sample correctly define a simple normal model with unknown mean and variance model function model function y y normal s 1 s poisson 1 return yendsample model function 10 smc 100 sampling multiple chainsturing supports distributed and threaded parallel sampling to do so call sample model sampler parallel type n n chains where parallel type can be either mcmcthreads or mcmcdistributed for thread and parallel sampling respectively having multiple chains in the same object is valuable for evaluating convergence some diagnostic functions like gelmandiag require multiple chains if you do not want parallelism or are on an older version julia you can sample multiple chains with the mapreduce function replace num chains below with however many chains you wish to sample chains mapreduce c gt sample model fun sampler 1000 chainscat 1 num chains the chains variable now contains a chains object which can be indexed by chain to pull out the first chain from the chains object use chains 1 the method is the same if you use either of the below parallel sampling methods multithreaded samplingif you wish to perform multithreaded sampling and are running julia 1 3 or greater you can call sample with the following signature using turing model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endendmodel gdemo 1 5 2 0 sample four chains using multiple threads each with 1000 samples sample model nuts mcmcthreads 1000 4 be aware that turing cannot add threads for you you must have started your julia instance with multiple threads to experience any kind of parallelism see the julia documentation for details on how to achieve this distributed samplingto perform distributed sampling using multiple processes you must first import distributed process parallel sampling can be done like so load distributed to add processes and the everywhere macro using distributed load turing using turing add four processes to use for sampling addprocs 4 initialize everything on all the processes note make sure to do this after you ve already loaded turing so each process does not have to precompile parallel sampling may fail silently if you do not do this everywhere using turing define a model on all processes everywhere model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend declare the model instance everywhere everywhere model gdemo 1 5 2 0 sample four chains using multiple processes each with 1000 samples sample model nuts mcmcdistributed 1000 4 sampling from an unconditional distribution the prior turing allows you to sample from a declared model s prior if you wish to draw a chain from the prior to inspect your prior distributions you can simply runchain sample model prior n samples you can also run your model as if it were a function from the prior distribution by calling the model without specifying inputs or a sampler in the below example we specify a gdemo model which returns two variables x and y the model includes x and y as arguments but calling the function without passing in x or y means that turing s compiler will assume they are missing values to draw from the relevant distribution the return statement is necessary to retrieve the sampled x and y values model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s return x yendassign the function with missing inputs to a variable and turing will produce a sample from the prior distribution samples from p x y g prior sample gdemo missing missing g prior sample output 0 685690547873451 1 1972706455914328 sampling from a conditional distribution the posterior treating observations as random variablesinputs to the model that have a value missing are treated as parameters aka random variables to be estimated sampled this can be useful if you want to simulate draws for that parameter or if you are sampling from a conditional distribution turing supports the following syntax model function gdemo x type t float64 where t if x missing initialize x if missing x vector t undef 2 end s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend construct a model with x missingmodel gdemo missing c sample model hmc 0 01 5 500 note the need to initialize x when missing since we are iterating over its elements later in the model the generated values for x can be extracted from the chains object using c x turing also supports mixed missing and non missing values in x where the missing ones will be treated as random variables to be sampled while the others get treated as observations for example model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend x 1 is a parameter but x 2 is an observationmodel gdemo missing 2 4 c sample model hmc 0 01 5 500 default valuesarguments to turing models can have default values much like how default values work in normal julia functions for instance the following will assign missing to x and treat it as a random variable if the default value is not missing x will be assigned that value and will be treated as an observation instead using turing model function generative x missing type t float64 where t lt real if x missing initialize x when missing x vector t undef 10 end s inversegamma 2 3 m normal 0 sqrt s for i in 1 length x x i normal m sqrt s end return s mendm generative chain sample m hmc 0 01 5 1000 access values inside chainyou can access the values inside a chain several ways turn them into a dataframe object use their raw axisarray form create a three dimensional array objectfor example let c be a chain 1 dataframe c converts c to a dataframe 2 c value retrieves the values inside c as an axisarray and 3 c value data retrieves the values inside c as a 3d array variable types and type parametersthe element type of a vector or matrix of random variables should match the eltype of the its prior distribution lt integer for discrete distributions and lt abstractfloat for continuous distributions moreover if the continuous random variable is to be sampled using a hamiltonian sampler the vector s element type needs to either be 1 real to enable auto differentiation through the model which uses special number types that are sub types of real or 2 some type parameter t defined in the model header using the type parameter syntax e g gdemo x type t float64 where t begin similarly when using a particle sampler the julia variable used should either be 1 a tarray or 2 an instance of some type parameter t defined in the model header using the type parameter syntax e g gdemo x type t vector float64 where t begin querying probabilities from model or chainconsider the following gdemo model model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endthe following are examples of valid queries of the turing model or chain prob x 1 0 y 1 0 model gdemo s 1 0 m 1 0 calculates the likelihood of x 1 and y 1 given s 1 and m 1 prob s 1 0 m 1 0 model gdemo x nothing y nothing calculates the joint probability of s 1 and m 1 ignoring x and y x and y are ignored so they can be optionally dropped from the rhs of but it is recommended to define them prob s 1 0 m 1 0 x 1 0 model gdemo y nothing calculates the joint probability of s 1 m 1 and x 1 ignoring y prob s 1 0 m 1 0 x 1 0 y 1 0 model gdemo calculates the joint probability of all the variables after the mcmc sampling given a chain prob x 1 0 y 1 0 chain chain model gdemo calculates the element wise likelihood of x 1 0 and y 1 0 for each sample in chain if save state true was used during sampling i e sample model sampler n save state true you can simply do prob x 1 0 y 1 0 chain chain in all the above cases logprob can be used instead of prob to calculate the log probabilities instead maximum likelihood and maximum a posterior estimatesturing provides support for two mode estimation techniques maximum likelihood estimation mle and maximum a posterior map estimation optimization is performed by the optim jl package mode estimation is currently a optional tool and will not be available to you unless you have manually installed optim and loaded the package with a using statement to install optim run import pkg pkg add optim mode estimation only works when all model parameters are continuous discrete parameters cannot be estimated with mle map as of yet to understand how mode estimation works let us first load turing and optim to enable mode estimation and then declare a model note that loading optim explicitly is required for mode estimation to function as turing does not load the opimization suite unless optim is loaded as well using turingusing optim model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endendonce the model is defined we can construct a model instance as we normally would create some data to pass to the model data 1 5 2 0 instantiate the gdemo model with our data model gdemo data mode estimation is typically quick and easy at this point turing extends the function optim optimize and accepts the structs mle or map which inform turing whether to provide an mle or map estimate respectively by default the lbfgs optimizer is used though this can be changed basic usage is generate a mle estimate mle estimate optimize model mle generate a map estimate map estimate optimize model map if you wish to change to a different optimizer such as neldermead simply place your optimizer in the third argument slot use neldermeadmle estimate optimize model mle neldermead use simulatedannealingmle estimate optimize model mle simulatedannealing use particleswarmmle estimate optimize model mle particleswarm use newtonmle estimate optimize model mle newton use acceleratedgradientdescentmle estimate optimize model mle acceleratedgradientdescent some methods may have trouble calculating the mode because not enough iterations were allowed or the target function moved upwards between function calls turing will warn you if optim fails to converge by running optim converge a typical solution to this might be to add more iterations or allow the optimizer to increase between function iterations increase the iterations and allow function eval to increase between calls mle estimate optimize model mle newton optim options iterations 10 000 allow f increases true more options for optim are available here analyzing your mode estimateturing extends several methods from statsbase that can be used to analyze your mode estimation results methods implemented include vcov informationmatrix coeftable params and coef among others for example let s examine our ml estimate from above using coeftable import statsbase to use it s statistical methods using statsbase print out the coefficient table coeftable mle estimate estimate stderror tstat s 0 0625 0 0625 1 0m 1 75 0 176777 9 8995 standard errors are calculated from the fisher information matrix inverse hessian of the log likelihood or log joint t statistics will be familiar to frequentist statisticians warning standard errors calculated in this way may not always be appropriate for map estimates so please be cautious in interpreting them sampling with the map mle as initial statesyou can begin sampling your chain from an mle map estimate by extracting the vector of parameter values and providing it to the sample function with the keyword init theta for example here is how to sample from the full posterior using the map estimate as the starting point generate an map estimate map estimate optimize model map sample with the map estimate as the starting point chain sample model nuts 1 000 init theta map estimate values array beyond the basicscompositional sampling using gibbsturing jl provides a gibbs interface to combine different samplers for example one can combine an hmc sampler with a pg sampler to run inference for different parameters in a single model as below model function simple choice xs p beta 2 2 z bernoulli p for i in 1 length xs if z 1 xs i normal 0 1 else xs i normal 2 1 end endendsimple choice f simple choice 1 5 2 0 0 3 chn sample simple choice f gibbs hmc 0 2 3 p pg 20 z 1000 the gibbs sampler can be used to specify unique automatic differentation backends for different variable spaces please see the automatic differentiation article for more for more details of compositional sampling in turing jl please check the corresponding paper working with mcmcchains jlturing jl wraps its samples using mcmcchains chain so that all the functions working for mcmcchains chain can be re used in turing jl two typical functions are mcmcchains describe and mcmcchains plot which can be used as follows for an obtained chain chn for more information on mcmcchains please see the github repository describe chn lists statistics of the samples plot chn plots statistics of the samples there are numerous functions in addition to describe and plot in the mcmcchains package such as those used in convergence diagnostics for more information on the package please see the github repository working with libtask jlthe libtask jl library provides write on copy data structures that are safe for use in turing s particle based samplers one data structure in particular is often required for use the tarray the following sampler types require the use of a tarray to store distributions ipmcmc is pg pmmh smcif you do not use a tarray to store arrays of distributions when using a particle based sampler you may experience errors here is an example of how the tarray using a tarray constructor function called tzeros can be applied in this way turing model definition model function bayeshmm y declare a tarray with a length of n s tzeros int n m vector real undef k t vector vector real undef k for i 1 k t i dirichlet ones k k m i normal i 0 01 end draw from a distribution for each element in s s 1 categorical k for i 2 n s i categorical vec t s i 1 y i normal m s i 0 1 end return s m end changing default settingssome of turing jl s default settings can be changed for better usage ad chunk sizeforwarddiff turing s default ad backend uses forward mode chunk wise ad the chunk size can be manually set by setchunksize new chunk size alternatively use an auto tuning helper function auto tune chunk size mf function rep num 10 which will profile various chunk sizes here mf is the model function e g gdemo 1 5 2 and rep num is the number of repetitions during profiling ad backendturing supports four packages of automatic differentiation ad in the back end during sampling the default ad backend is forwarddiff for forward mode ad three reverse mode ad backends are also supported namely tracker zygote and reversediff zygote and reversediff are supported optionally if explicitly loaded by the user with using zygote or using reversediff next to using turing for more information on turing s automatic differentiation backend please see the automatic differentiation article progress loggingturing jl uses progresslogging jl to log the progress of sampling progress logging is enabled as default but might slow down inference it can be turned on or off by setting the keyword argument progress of sample to true or false respectively moreover you can enable or disable progress logging globally by calling turnprogress true or turnprogress false respectively turing uses heuristics to select an appropriate visualization backend if you use juno the progress is displayed with a progress bar in the atom window for jupyter notebooks the default backend is consoleprogressmonitor jl in all other cases progress logs are displayed with terminalloggers jl alternatively if you provide a custom visualization backend turing uses it instead of the default backend", "title": "Guide"},{"location": "/docs/using-turing/index", "text": "turing documentationwelcome to the documentation for turing introductionturing is a general purpose probabilistic programming language for robust efficient bayesian inference and decision making current features include general purpose probabilistic programming with an intuitive modelling interface robust efficient hamiltonian monte carlo hmc sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and compositional inference via gibbs sampling that combines particle mcmc hmc and random walk mh rwmh", "title": "Turing Documentation"},{"location": "/docs/using-turing/performancetips", "text": "performance tipsthis section briefly summarises a few common techniques to ensure good performance when using turing we refer to the julia documentation for general techniques to ensure good performance of julia programs use multivariate distributionsit is generally preferable to use multivariate distributions if possible the following example model function gmodel x m normal for i 1 length x x i normal m 0 2 endendcan be directly expressed more efficiently using a simple transformation using fillarrays model function gmodel x m normal x mvnormal fill m length x 0 2 endchoose your ad backendturing currently provides support for two different automatic differentiation ad backends generally try to use forwarddiff for models with few parameters and reversediff tracker or zygote for models with large parameter vectors or linear algebra operations see automatic differentiation for details special care for tracker and zygotein case of tracker and zygote it is necessary to avoid loops for now this is mainly due to the reverse mode ad backends tracker and zygote which are inefficient for such cases reversediff does better but vectorized operations will still perform better avoiding loops can be done using filldist dist n and arraydist dists filldist dist n creates a multivariate distribution that is composed of n identical and independent copies of the univariate distribution dist if dist is univariate or it creates a matrix variate distribution composed of n identical and idependent copies of the multivariate distribution dist if dist is multivariate filldist dist n m can also be used to create a matrix variate distribution from a univariate distribution dist arraydist dists is similar to filldist but it takes an array of distributions dists as input writing a custom distribution with a custom adjoint is another option to avoid loops ensure that types in your model can be inferredfor efficient gradient based inference e g using hmc nuts or advi it is important to ensure the types in your model can be inferred the following example with abstract types model function tmodel x y p n size x params vector real undef n for i 1 n params i truncated normal 0 inf end a x params y mvnormal a 1 0 endcan be transformed into the following representation with concrete types model function tmodel x y type t float64 where t p n size x params vector t undef n for i 1 n params i truncated normal 0 inf end a x params y mvnormal a 1 0 endalternatively you could use filldist in this example model function tmodel x y params filldist truncated normal 0 inf size x 2 a x params y mvnormal a 1 0 endnote that you can use code warntype to find types in your model definition that the compiler cannot infer they are marked in red in the julia repl for example consider the following simple program model function tmodel x p vector real undef 1 p 1 normal p p 1 x normal p 1 endwe can useusing randommodel tmodel 1 0 code warntype model f random global rng model turing varinfo model turing samplefromprior turing defaultcontext model args to inspect type inference in the model reuse computations in gibbs samplingoften when performing gibbs sampling one can save computational time by caching the output of expensive functions the cached values can then be reused in future gibbs sub iterations which do not change the inputs to this expensive function for example in the following model model function demo x a gamma b normal c function1 a d function2 b x normal c d endalg gibbs mh a mh b sample demo zeros 10 alg 1000 when only updating a in a gibbs sub iteration keeping b the same the value of d doesn t change and when only updating b the value of c doesn t change however if function1 and function2 are expensive and are both run in every gibbs sub iteration a lot of time would be spent computing values that we already computed before such a problem can be overcome using memoization jl memoizing a function lets us store and reuse the output of the function for every input it is called with this has a slight time overhead but for expensive functions the savings will be far greater to use memoization jl simply define memoized versions of function1 and function2 as such using memoization memoize memoized function1 args function1 args memoize memoized function2 args function2 args then define the turing model using the new functions as such model function demo x a gamma b normal c memoized function1 a d memoized function2 b x normal c d end", "title": "Performance Tips"},{"location": "/docs/using-turing/quick-start", "text": "probablistic programming in thirty secondsif you are already well versed in probabalistic programming and just want to take a quick look at how turing s syntax works or otherwise just want a model to start with we have provided a bayesian coin flipping model to play with this example can be run on however you have julia installed see getting started but you will need to install the packages turing and statsplots if you have not done so already this is an excerpt from a more formal example introducing probabalistic programming which can be found in jupyter notebook form here or as part of the documentation website here import libraries using turing statsplots random set the true probability of heads in a coin p true 0 5 iterate from having seen 0 observations to 100 observations ns 0 100 draw data from a bernoulli distribution i e draw heads or tails random seed 12 data rand bernoulli p true last ns declare our turing model model function coinflip y our prior belief about the probability of heads in a coin p beta 1 1 the number of observations n length y for n in 1 n heads or tails of a coin are drawn from a bernoulli distribution y n bernoulli p endend settings of the hamiltonian monte carlo hmc sampler iterations 1000 0 05 10 start sampling chain sample coinflip data hmc iterations plot a summary of the sampling process for the parameter p i e the probability of heads in a coin histogram chain p", "title": "Probablistic Programming in Thirty Seconds"},{"location": "/docs/using-turing/sampler-viz", "text": "sampler visualizationintroductionthe codefor each sampler we will use the same code to plot sampler paths the block below loads the relevant libraries and defines a function for plotting the sampler s trajectory across the posterior the turing model definition used here is not especially practical but it is designed in such a way as to produce visually interesting posterior surfaces to show how different samplers move along the distribution env gks encoding utf 8 allows the use of unicode characters in plots jlusing plotsusing statsplotsusing turingusing bijectorsusing randomusing dynamicppl getlogp settrans getval reconstruct vectorize setval set a seed random seed 0 define a strange model model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s bumps sin m cos m m m 5 bumps for i in eachindex x x i normal m sqrt s end return s mend define our data points x 1 5 2 0 13 0 2 1 0 0 set up the model call sample from the prior model gdemo x vi turing varinfo model convert the variance parameter to the real line before sampling note we only have to do this here because we are being very hands on turing will handle all of this for you during normal sampling dist inversegamma 2 3 svn vi metadata s vns 1 mvn vi metadata m vns 1 setval vi vectorize dist bijectors link dist reconstruct dist getval vi svn svn settrans vi true svn evaluate surface at coordinates function evaluate m1 m2 spl turing samplefromprior vi svn m1 vi mvn m2 model vi spl getlogp vi endfunction plot sampler chain label extract values from chain val get chain s m lp ss link ref inversegamma 2 3 val s ms val m lps val lp how many surface points to sample granularity 100 range start stop points spread 0 5 start minimum ss spread std ss stop maximum ss spread std ss start minimum ms spread std ms stop maximum ms spread std ms rng collect range start stop stop length granularity rng collect range start stop stop length granularity make surface plot p surface rng rng evaluate camera 30 65 ticks nothing colorbar false color inferno title label line range 1 length ms scatter3d ss line range ms line range lps line range mc viridis marker z collect line range msw 0 legend false colorbar false alpha 0 5 xlabel ylabel zlabel log probability title label return pend samplersgibbsgibbs sampling tends to exhibit a jittery trajectory the example below combines hmc and pg sampling to traverse the posterior c sample model gibbs hmc 0 01 5 s pg 20 m 1000 plot sampler c hmchamiltonian monte carlo hmc sampling is a typical sampler to use as it tends to be fairly good at converging in a efficient manner it can often be tricky to set the correct parameters for this sampler however and the nuts sampler is often easier to run if you don t want to spend too much time fiddling with step size and and the number of steps to take note however that hmc does not explore the positive values very well likely due to the leapfrop and step size parameter settings c sample model hmc 0 01 10 1000 plot sampler c hmcdathe hmcda sampler is an implementation of the hamiltonian monte carlo with dual averaging algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader c sample model hmcda 200 0 65 0 3 1000 plot sampler c mhmetropolis hastings mh sampling is one of the earliest markov chain monte carlo methods mh sampling does not move a lot unlike many of the other samplers implemented in turing typically a much longer chain is required to converge to an appropriate parameter estimate the plot below only uses 1 000 iterations of metropolis hastings c sample model mh 1000 plot sampler c as you can see the mh sampler doesn t move parameter estimates very often nutsthe no u turn sampler nuts is an implementation of the algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader nuts tends to be very good at traversing complex posteriors quickly c sample model nuts 0 65 1000 plot sampler c the only parameter that needs to be set other than the number of iterations to run is the target acceptance rate in the hoffman and gelman paper they note that a target acceptance rate of 0 65 is typical here is a plot showing a very high acceptance rate note that it appears to stick to a mode and is not particularly good at exploring the posterior as compared to the 0 65 target acceptance ratio case c sample model nuts 0 95 1000 plot sampler c an exceptionally low acceptance rate will show very few moves on the posterior c sample model nuts 0 2 1000 plot sampler c pgthe particle gibbs pg sampler is an implementation of an algorithm from the paper particle markov chain monte carlo methods by andrieu doucet and holenstein 2010 the interested reader can learn more here the two parameters are the number of particles and the number of iterations the plot below shows the use of 20 particles c sample model pg 20 1000 plot sampler c next we plot using 50 particles c sample model pg 50 1000 plot sampler c", "title": "Sampler Visualization"}]}
